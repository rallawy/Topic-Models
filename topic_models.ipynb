{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ3AvU6aabQj"
   },
   "source": [
    "# Topic Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8VitD_JcabQn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# libraries that we need\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "import re # regular expression python\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e5HjhpTTabQp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# LOAD THE INPUT FILE DH_CollectingData2022_review.tsv \n",
    "input_file = 'phrases_dataset.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBEpypvzabQq"
   },
   "source": [
    "## Exploring the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tL-n9BXcabQr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2a50deec-d724-4a53-d972-751af263cfcf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  For Nik, he only wants to silence the cacophon...    0.0\n",
       "1                          \"I can play this two ways    0.0\n",
       "2  Mild, because it isn't conclusive, and doesn't...   -1.0\n",
       "3  You can also get some more information about t...    0.0\n",
       "4  Soon, Hero, who has never had friends, is thru...    0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening the file using pandas and storing it into a dataframe \n",
    "dataset = pd.read_csv(input_file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, header=None)\n",
    "dataset.columns =['sentence', 'score']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk6TvEQRJeK0"
   },
   "source": [
    "Create a data series that contains all the sentences in the dataset and remmove all characters that are not alphanumeric or underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rs3B-0t0JeK0",
    "outputId": "16fb6a27-93e5-41f4-b39e-d149c5559963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For Nik  he only wants to silence the cacophony of sounds and colors he sees and for Fiona  it is the hope of a new life in Costa Rica', ' I can play this two ways', 'Mild  because it isn t conclusive  and doesn t give us the information we need to fully appreciate the story that Lloyd is telling', 'You can also get some more information about the books and writing in my exclusive interview with author Belinda Crawford at  https   goo', 'Soon  Hero  who has never had friends  is thrust in a school where she quickly has to decide who are her friends and who aren t', 'I did like Steven  or Stephen  I listened to the book', 'The plot is quick moving and the action is violent', 'Loved everything about this book', 'What happened to the sorceress to make her hate people so  What is her purpose in locking up the girl  Does that somehow ensure her possession  or the girl s purity  If so  why is purity important to her ', 'Great  quick read', 'Although there isn t character development  as is the case with most mystery novels  yet the reader comes out enlightened about several notions and ideas in life', 'I liked the ending ', 'Going in I really liked it but unfortunately left me bored and infuriated at the end   which I think is really hard to do  This is the first book I finished only because I wanted it over with', 'The wolf loved chasing the sheep around and around', 'I m not giving this 5 stars because the big reveal kind of disappointed me in that it s a classic case of possible  but not really probable', 'Collected in this volume are three short stories set in the same world as his first novel  The Crescent Throne  as well as a western  sci fi  bunnies with swords  and an urban fantasy', 'I believe Barnes delivers on that promise in this book   the Garden of Stones was well worth the read', 'eh I hate how the author made Duke from a nice guy to a complete dick wad and Trevor into mr', 'And if you finish Pig Island and really like the ending  read Hanging Hill', 'Cait and Bud travel to Amsterdam to fulfil Jonas  will and uncover a mystery if there is one', 'The narrator is good and the audiobook is only about seven hours', 'I guess I didn t track this on goodreads when I first read this so I ll just rate it 3 stars  but I have to reread it ', 'But it would have been so much more interesting if Hill had gone with the more simple  logical  and real explanation  Lee doesn t react to the horns because Lee has no guilt', 'Great story of a girl s friendship with her dog ', 'I have an empty house that reeks of death', 'I have read all her books and I admire her', 'I love anything that incorporates Shakespeare and that was clever  but the main issue that turned me off is the 12 Step focus', 'It did have an interview with Green after the book  which some readers might find interesting ', 'There were a lot of things in this book that I didn t expect', 'Can I just say', 'My 8 year old loved it', 'I very much liked the main characters and never questioned how they acted and reacted in such a bizarre situation  which is impressive', 'I don t know  this atmosphere of things not being what they appear to be', 'I spent the first half of this book getting to know a dozen likable  eclectic characters and the second half yelling at the car radio and gripping the steering wheel like it was a cut glass doorknob and I was swinging into space', 'I think that another reason I enjoyed it so much was that it reminded me a bit of P', 'If you read Hayder s Hanging Hill and enjoyed the ending  and maybe yelled at the book at little bit   then read Pig Island', 'But I would not be honest if I said I enjoyed this read', 'Cathy Ace expertly weaves in comic relief also from the first few pages to relieve the tension she built on the first page of the novel', '  I wonder if someone s watching through one of the pictures on the wall ', 'I found it very difficult to buy into the world the author was creating when I first started reading this book', 'It doesn t last forever', 'Maybe you know what I mean', 'All in all a good first novel and I am planning on reading the follow up as the story is pretty solid and the author kept it intriguing enough in general to enjoy the adventure ', 'The Corpse with the Garnet Face tackles art  lots of it  since Jonas and his friends were artists and painters', 'This is the first book I completed for my Goodreads 2013 reading challenge  and I can say that the bar has been set so low that it can only get higher', 'I love you so much ', 'The characters aren t really heroes or heroic  and yet by the end you understand what makes them tick  mostly   and why they are the crappy examples of morality that they are', 'I ve liked several of his other novels better ', 'Cal is part monster  part human and all sarcasm', 'This book went from light hearted to heavy actionactionactionaction and the main character s life went from pretty awesome to complete SHIT in like 2 short chapters', 'It s also the first novel I have read fully on my iPad which is saying something as I much prefer the feel of a book in my hand and all other novels on my iPad have remained unread', 'Barnes presents in The Garden of Stones a well developed  fully laid out world', 'Readers of the paper edition might have seen it  by accident  when going to set the book down', 'Just the journey took up most of the book and the final confrontation was kind of short', 'Spell binding pageturner ', 'Clever story  witty and with a likeable main character', 'Through a series of twists and turns  August creates a situation where Nicole will become his blood mate  but she doesn t give in so easily  and is a very strong and feisty heroine', 'It fits her  pretty and carefree as she is', 'You have to climb the stairs  lol  and make the first move', 'Readers of the electronic edition  though  would suffer the entire length of the book before discovering that hidden away at the back there is a mild attempt at explaining who s who', 'Of course  I thought maybe it was Marston because he was the first to go  but alas I was wrong', 'I ve read wayyy too many books where the author turns one of the guys into a dick head so it gives the main character an excuse to fall in love with the  right guy ', 'Also  the audiobook has a really nice afterword by Hill himself  where he talks about the writing of this book and about audiobooks  and he gives some audio recommendations ', 'The wolf didn t know how to read', 'As with any trilogy  this book was more about the getting from point A  book 1  to point B  book 3  presumably  where there is resolution   full of nice bits of action and the building up of tension for the third book ', 'Like many good fantasies  Abercrombie doesn t try and hide the grit behind feats of magic and enchanted swords', 'Part leopard  part rat  Fink  a rucpard  speaks in third person and is absolutely adorable despite being 600 kilos in weight and very defensive of his charge  Hero', 'The wolf tells the frog all the things he eats  cute  fluffy animals', 'Just because you re a monster doesn t mean you can t do some good ', 'This is a solid police procedural and a good start to a series', ' I was going to jail', 'Great nonfiction for older kid story times ', 'A magician is wrongfully imprisoned for the death of his partner and vows to take revenge on the children who were truly to blame', 'There was no sexual tension among any of the main characters', 'The Kavach building is certainly a mystery to be solved  but I had no clue as to what was waiting for me on the other side of room 14', 'A great little book all around ', 'HATE', 'I felt the beginning was kinda slow', 'And if you re reading it alone  just as an adult who loves Snicket stories  then you would have the fun of the illustrations and listening to Terry Gross read a story about a kidnapped newt', 'I assume it was meant to be funny but it seemed low brow and childish and detracted from an otherwise solid and affecting story', 'I will admit that I enjoyed how she ended this book and I loved how she made readers think hard about Sebastian and his truthful plans', 'Maybe it s just me but I was annoyed at how Ethan had to mention his undying  unyielding  eternal love for Lena practically EVERY', 'La Donna  an Italian spy known for her subterfuge and intrigue and  of course  inescapable and beguiling beauty  possesses information of a plot against the King and is willing to share it  for a price', 'And I LOVE DEX ', 'To sum up  because we don t waste too many words  Horns is the story of a young man that wakes up one morning  a year after his love was raped and murdered  to discover a pair of horns growing out of his skull', 'I m hooked  Can t wait to read rest of series', 'He worried me during the first ten minutes  I thought he might a little too Hollywood', 'I totally did NOT see it coming   I did  however  predict the ending even before reading the book', ' You can tell them who you are  or what your job in the book was', 'I knew that would happen since before even reading the book  from when I just read the synopsis', 'Before the novel even gains too much traction  his father and brother are killed  leaving him the title of king', 'The narrator  Ray Porter  was great', 'Two of the three main characters are girls  and there s not a vampire fang  dragon wing  or magic potion in sight  which is so refreshing', 'I felt it was a bit like a nicely written history book', 'The story merges a post apocalyptic future with a Caribbean voodoo context  and the characters are multi generational and well written', 'She has a strong sense of what is right and what is wrong  even if that means that her actions will hurt her', 'Helena is a very unlikely heroine  prickly  moody and basically angry at the world', 'Delaney during the first section of the book', 'Once again  I won t soon forget the characters created by Emma Scott', 'I HOPE IM WRONG', 'I scanned a review that was likening it to Lost  and while I loved the first couple seasons of that show  I think it would be a disservice to compare 14 to something that wasn t well thought through', 'Live out loud', 'Rich girl  poor girl Russian folklore   I really liked it ', 'I was constantly wondering who would die next and who was committing the murders', 'Horns fit the bill  and I m glad I read it', 'I became a fan of her when I read her story in Foretold and I m excited to read more books by her ', 'But he s not a perfect writer and neither is his son', 'Because it was split into two parts  six months of the story separating each section   it felt as though I d finished two books', 'I loved this book', 'A mystery that produces more mysteries and keeps the reader s mind working', 'Loved the story', 'It was very noir  in a lot of ways', 'He s unprepared for the role  and being born with less than a full compliment of fingers on one hand  he is quick to be given the titular half a king', 'BUT LUCAS NOW I JUST WANNA PUNCH YOUR FACE OUT', 'I could literally stare at your lips for days and never get bored', '75 stars', 'Beautiful illustrations and good mix of technical detail with interesting bits about the lives of the Wright Brothers  the two middle brothers of five siblings', 'Once again Colleen Hoover s love stories leave me speechless and emotional', 'The novel was fairly paced', 'She even keeps her long time agent Kate at arm s length', 'AWESOME', 'But they were just so perfect and wonderful and marvelous that it was all a little tooth rotting  really', 'At first  the cast of characters seems daunting', 'This book is also a fascinating glimpse into the lonely life of a writer and the almost obsessive need to get the words out at all costs', 'They re what I used to be  they re Tom and me five years ago', 'this was a cool story  loved the main character   Jack', 'She turned to Norah', 'HAH  I knew it  I predicted that Seth would turn on Alex and become power hungry etc', ' Reviewed from an ARC obtained from NetGalley', 'But it s so much more than that because of the way Lindqvist treats his characters and the way he navigates the narrative', 'Highly recommend  ', 'I was pretty disappointed', 'It seems to me that Changes has left it s mark on the series and this is the end of Harry Dresden Chicago Wizard investigator  and the start of a new series Harry Dresden Superpowered Wizard', 'I give it like 2', 'The main character reminded me a bit of Trixa from Rob Thurman s Trickster series', 'Pressed to Death by Kirsten Weiss starts with the above lines  the cozy  possibly paranormal mystery novel is exciting  quick paced and fun', 'While I m often picky about the sci fi books I read  Hero was definitely up my alley', 'Simply stunning   Emma Scott writes THE most lyrical and gorgeous fiction', ' On rare days  Kate regrets ever meeting the woman', 'Exciting  Hero by Belinda Crawford is quick paced and to the point', 'This trilogy certainly doesn t get the popular attention it deserves', 'It helps that Webster reads a little like a 1920 s Raylan Givens  that Arliss Howard has a low  melodic  softly twangy voice  and that Elmore Leonard always writes a fast  fun story', 'I really wanted it to be a book instead of this separate tales', 'I enjoyed the story very much ', 'Photos are mismarked  the research is poor', 'They walk around and around their fenced in field until they meet a wolf in sheep s clothing', 'The direction it took  the mature content and themes   The emphasis on the psychological and physical torture that was happening    that one thing Adrian and Sydney get married  HOLY CRAP   I literally yelled   WHAT   out loud when I read this part', 'They enjoy themselves and head to bed', 'What begins as a one night stand with zero expectations for the future turns into a burning hot love and a deep  unbreakable connection', 'I really liked this story', 'Another excellent story from S', 'Reluctantly  Wooliam agreed', 'Only Kitty Thomas could create a cast of strong characters like those in Blood Lust and an fantastically creative story that will keep you guessing until the very end', 'and I drove up to Door County and back', 'It made Hazel come off as a prig  which I m sure wasn t the intention and might have unfairly colored my view of her', 'There are many mysteries surrounding Hero and her family  as well as her new nanny  or as Crawford calls her  minder ', 'I d have liked more descriptive details about the city itself at the very start of the story', 'Boys and girls are sure to enjoy ', 'Never in a million years did I think I would love a book about a parallel world with elves  kings  magic', 'Only not that good', 'Soon enough they begin to feel that the accounts given to them of Jonas are contradictory and that some of the friends seem to be hiding something', 'The only part of the book that was interesting for me was the last 100 pages  Nook eBook edition   but that s just me', 'The reader is catapulted into the conflict head first  and although those first few chapters are confusing  chaotic scenes of fighting between factions in a war we don t understand  it all makes sense soon enough', 'It was provided in exchange for an honest review ', 'Because we don t last forever', 'Curious', 'Those last like 20 pages were full of twists and turns like I m still not sure what just happened', 'Hoover has actually made me fall more in love with Holder than I already was ', 'And it d be worth it to stay up past your bedtime to get it finished ', ' ARC provided by the author in return for an honest review', 'An excellent and terrifying book', 'She doesn t want to rely on anyone at all', 'Narrated from the first person perspective of Cait Morgan  the novel is a fun  mind provoking cozy mystery', 'I was not a girl  and my parents did not promise me away  so far as I know  so this story had no message for me', 'Eliza grimaced', 'My biggest beef is with the reason Lee Tourneau is impervious to Ig s horns at the start', 'But on the whole  it s a fast read and a good slice of brain candy thriller ', 'The concept of this book was a great idea but I was disappointed by the book in general ', 'Just curious ', 'I must have gone through a box of tissues reading this', 'Although it covers a period of two days  still I felt the ship s passengers were doing  a lot  in those two days', 'i dont get the obsession some people have with this book xD Nothing interesting appened till the last 50 pages and then it ended before w barely knew anything xD', 'Archie s unbearably creepy obsession with Gretchen Lowell just gets creepier  and it makes for irritatingly good reading', 'But Hill s take on and exploration of the devil and evil is original and interesting  so I was okay with some of my minor issues ', 'Twisting 3 Kingdoms together in this great adventure due to be released in December and definite must for all lovers of medieval but also just a good page turner   ', 'A fun and exciting adventure featuring all of your favourite  Princes Charming  and their Princesses', 'Maybe Helena s madness just took longer to come out', 'I think we have this idea that it s bad  the way dudes are always thinking about sex', 'The characters inhabit a sparse  semi arid landscape that is nearly devoid of other human life', 'However  the over the top cruelty of Reed and Ella s fellow students here turned me off', 'If you like lots of high school drama  then this book is for you  Broken Prince reminded me of Gossip Girl in many ways', 'It is a very cute take on Jane Austen s  Pride and Prejudice   set amid the privileged offspring of Hollywood royalty', 'But I feel it s almost unfortunate for those few authors who have talent and didn t just spew out filth to be included with the rest ', 'So  cute    Sophie doesn t want to eat the squash bought at the market but to keep it as her friend and companion   all is well until little squash starts getting some brown spots   sweet and fun  ', 'Too much information  especially when revealed in a story utilizing the third person omniscience of the all knowing  all seeing  all explaining author  lends a detachment to the reader that makes it difficult to invest fully in the characters', 'Not so much that it was supposedly a cliffhanger  because to me  it honestly wasn t  but just the ending itself', 'I love the way she makes you feel you truly are down  south   ', ' I m not going to interfere', 'Based on a recommendation from the DanaSquare blog  I picked up this book', 'But it changes us all', 'must check it out now ', 'So it wasn t good enough to get published on its own  he had to publish something better  presumably  and build a name for himself', 'I agree with the other reviewer who said it had far too much going on', 'She was right that the first pages make it feel as though you ve jumped into a sequel without much of an explanation of who the characters are and what sort of a world they re living in', 'Guess which one I m doing  ', 'But this lovely book  and my constant search for new reading material for my children  sparked my curiosity', 'In particularly  I kind of enjoyed the fact that the Devil can be a superhero in the right light ', 'Names  species  factions  how will you keep it all straight  While Barnes  world is fully realized  it is a departure from the familiar', 'First  the list of characters  in addition to being too long to keep track of in such a short book  was tucked away at the back of the book', 'The girl exuded a calm that danced along my skin  and when I inhaled  it was like breathing in the scent of something delicious but far away', 'I m not sure they needed 4 pages to show all the animals laughing in the end', 'You will not be disappointed', 'As I read the first five or so pages  I thought to myself   Lois Duncan isn t really a great writer   but then as I got about five more pages in I realized that  no  Duncan isn t a great writer  but she can tell one hell of a story', 'They re happy  I can tell', 'As it s a collection  I can t paint all authors included with the same brush', 'So why the struggle up to three stars  This book failed for me on purely technical merits', 'While slow to start  beginnings  although not exposition full  are a little awkward slow to build   Sanderson provides a stunningly well thought out magic system  something he s known for and for good reason   as well as high paced  well written fantasy action', 'Not the same impact it created on me on the first book ', 'If anything this book is a quick  enjoyable read that will keep you on the edge of your seat ', 'The reason this is three stars instead of four is that I listened to the audiobook instead of reading the hard copy', '5 really enjoyed it ', 'It s fairy tale enough for kids  but bloody enough for us fans', 'I can t ', 'The focus of the story changes to the sorceress and her charge', 'Living like this  the way I m living at the moment  is harder in the summer when there is so much daylight  so little cover of darkness  when everyone is out and about  being flagrantly  aggressively happy', 'Amazing   Higginson is now one of my most admired authors', ' So it s charity  ', 'When we first meet Ebba  she describes her brother  saying  Jonas was a bad boy   whereas when we begin to meet Jonas  friends in the Netherlands  some say  He was a good man', 'As a reader  you have this perspective of watching Jorg take bite after painful bite from his heart  because it is bitter  and it is his', '5   5 GORGEOUS STARS', 'Even though Rachel is COMPLETELY unlikable  I was cheering for her throughout the book', 'While I think Hill played the devilish references a bit much  from the Evil Knieval trail to the number of times he throws 666 out there   the book was still a fun read and well worth the price of admission', ' My mother used to tell me that I had an overactive imagination', 'Then they meet a wolf', 'To be even more honest  I did not finish it and merely skimmed', ' The Alchemist in the Shadows  is a good read  but you may have to', 'But he was actually perfect and did great voices for a variety of characters  some with accents', 'I d drop this by half a star if I could', 'I plan to recommend it to my creative writing students  who love this genre but don t have much experience drawing characters and integrating conflict and the world they ve created ', 'The first chapter sets the tone for the kind of book you have started', 'It s a book about growing up  about family  about love  and about loss', 'I began to think that the dreams were real and wondered at first whether there was some paranormal element to the novel', 'The camper almost goes down a 50 foot waterfall  but the bear thinks the hitch is a marshmallow  so he grabs it and pulls it to shore before realizing it was a dirty  less than tasty  hitch instead', 'The suspense that it kept you in was great', 'Immediately she is swept off to marry the king in an arranged marriage', 'Once I started I could not put it down', 'From the prologue I immediately fell in love with Ms', 'This book is all about female empowerment while also tackling many subjects such as racism  stereotyping  and sexuality', 'All of the ingredients for a great read ', 'It s a fun  fantastical  mystery  full of oddball characters  a bearded lady  a time traveling ugly man who may or may not have founded London  two assassins dressed like overgrown prep school drop outs  a human fly  and  of course  the titular side kick  the Somnambulist', 'I think this could make a good read aloud grade 4 6', 'Markus Zusak made a post about how he enjoyed this book', 'Thank you to both parties for the opportunity to read this book in advance   it s been hard holding this review until closer to publication   ', 'I ve read it about 50 times to said 2 year old  who loves to pick her nose ', 'The characters were somewhat stereotypes and I did not find myself attached to any of them', 'Buy it', 'Cait has a special skill that has enabled her to solve crimes in the past', 'It only took about 6 hours to listen to  so I m imagining that reading it outright would only take one good night', 'Wooliam got out his book and showed the wolf', 'She makes it through a really tough begin hopefully everything works for her in the next book ', 'Despentes has her own voice and she is writing about important things in life', 'It made some of Hill s cheesier dialogue even cheesier', 'my final story will come out  and everyone will know the truth', 'I m still unsure whether or not I understood the book in its entirety  and I m still thinking about it', 'maybe I m missing something ', 'She stalks her ex husband and his new wife Anna but just can t seem to stop herself', 'Time passed and the world turned  as they say  and I gave the book another shot', 'In GHOST  we see a lot of character background  which helps the reader form a better picture of each character', 'I would definitely like to read more about the characters in this story', 'Our understanding of the world is so incomplete we are left questioning what is happening for part of the book', 'I did  however  feel that the conclusion to The Corpse with the Garnet Face was rather confusing  even as Cait laid out all their findings  I still felt lost', 'This book is an excellent example of how you can do a really good third person mystery', 'Well I liked it', 'The wolf looked anywhere except at the pages of words', 'Of course  the truly brave writer   and this is why epic fantasy is known for its lengthy tomes  I think   will layer their explanations and insights about their fantasy world through the telling of the novel  so that while you may start not knowing who Bahl is or what a white eyes is  by the end of the story you ll catch yourself swearing in Bahl s name and cursing the blight that is a white eye born man', 'It feels good too', 'This story takes place after Skin Game  book 15 ', 'It s dark  and every page drips with what s not being said  and the tension of knowing that someone  probably everyone  is lying is thick  thick  thick', 'Really aggravated and disappointed but happy that I finally finished reading and got it over with ', 'This book was on a Flavorwire list of books you won t be assigned in high school  so I decided to check it out', 'The characters are well developed  especially Hero  who soon realises there is so much more to her being  special ', 'I wanted to be Maggie  and I think of the book often ', 'The reader meets three very different women and learns about their lives  failures  issues  successes and desires', ' If you lie enough times  no one believes your truth', 'What this book did extremely well was create tension and suspense', 'Indris  the warrior scholar and principal character', 'This character development and story groundwork are essential for the rest of the book s tension and meticulous plotting', 'Although I really liked how DeMille s character grew from the previous novel he appeared in  I it took me awhile to really fall into the book and become engaged in the story', 'After coming home  I re read it', ' I loved this book', 'Got this for free  Read it on my iphone', 'I needed something that was fun to read  knew not to take itself too seriously  and was outside my usual reading circles without being just foreign', 'Alessandra Torre deserves all the accolades for this one ', 'There were a lot of absolutely unbelievable plot points  but I didn t mind so much', 'Over all  it was average', 'The book only got interesting on the part where Cricket started appearing', 'Particularly the stories by  B', 'There were a lot of beautiful lines and images in the novel', 'This is a fairy tale he wrote for his teenage daughter', 'This was a well written  easy to read debut novel', 'Note  I received an Advanced Readers  Copy  ARC  of The Corpse with the Diamond in exchange for an honest review as part of a blog tour for the book via Lori s Great Escapes Virtual Book Tours', 'The best part  I ve taken so long to read this book that the sequel is already out  so if you enjoy it  you can keep reading ', 'Since the novel is narrated from Cait s perspective  we get her first impressions on everything', 'He then leaves her alone in the middle of the woods and so begins this epic tale that had me swooning at every word', 'I like people who read', 'One sitting tears awesome ', 'Simply dropping in a reference to someone s cell phone in one scene doesn t fix this  it only highlights the absence of technology in every other scene', 'Why does the expectant mother crave the herb rapunzel in her neighbor s garden  Why does the husband agree to give the sorceress their child  Fear  Disbelief  His exchange of an immediate pleasure for a future punishment is the baseline human instinct  and it is shown up to be bad', 'It s his ride  and I loved being on it', 'I feel like there was a lot left unexplained', 'But she sees a talent there that would be perfect for her own story', 'There are many great twists and turns', 'I felt his pain and the pressure he was under to try and protect the innocent Ella at all costs', 'It s a super fast read  and the ending as absurd as the premise  but somehow that s okay  the absurdity is so over the top that you stop caring and just go with it', 'I think that in this setting and in this story a sociopath makes a really interesting character  and one I d have liked explored just a bit further ', 'BUT there are SO many obstacles standing in their way ', 'I d certainly read it ', 'She is pure Americana  rough and raspy  and a great hero and even better villain', 'I guess that could be interpreted as a good way to bring the reader back for the following books', 'The writing duo that makes up Erin Watt are incredibly talented  and I love their other books', 'I thought it was going to go off in a direction I wouldn t enjoy when I discovered the antagonist but I was pleasantly surprised', 'The uncle  Jonas  leaves a letter for Bud and gives him a task to uncover a long dead mystery that took place early in Jonas  life', 'The best book in the bloodlines series so far in my opinion', 'Many lambasted this novel as dark and misogynistic  but I have to admit that I didn t see any of that when I read it', 'Every time I felt I had a grasp  we were introduced to a new character that made me question whether we were speaking the same language', 'His descriptions and use of novelistic elements to tell this story work beautifully and bring out more dimensions than another writer would have gotten to ', 'This is most definitely not a romance but the main characters are both renowned romance writers', 'I m looking forward to the rest ', 'I  think  I read this when I was a kid  but I can t remember', 'And please don t be put off by this being the second book in a series', 'Savage Delight begins right where Book 1  Lovely Vicious  ends', 'Or possibly it s because I wanted more of something    more development of Oaksey  or of his wife  Lexie  or more time spent on the island with the villagers', 'This is a tale of revenge  as rich in its single mindedness as any classic by Dumas', 'In particularly  I kind of enjoyed the fact that the Devil can be a superhero in the right light ', 'But if you are looking for a summer pool or beach read  then pick up Broken Prince and enjoy the crazy ride ', 'Some sass and strength and a lot of humanity in a non human', 'And when they meet  it is pure magic', 'Ohlsson  though  seems to have a fascination with unfaithful husbands  they re all over the place  and each is dealt with in a different way  which is interesting if done as a character study  but there s not enough detail in each relationship  so they re just sort of', 'But be warned  there is another cliffhanger ', 'Crawford has done a tremendous job crafting her world building and setting', 'I ve tried to read a few Sanderson books  but this is the first I ve actually followed through all the way', ' Beautiful sunshine  cloudless skies  no one to play with  nothing to do', 'To everyone on board  it appears that Tommy died of natural causes  but to the mystery solving psychoanalyst  the man appeared to have been poisoned', 'Like  OKAY  WE GET IT ETHAN', 'I m looking forward to Cornell s next book about this group ', 'As a French adventure story that is supposed to remind us of Dumas   if Alexandre had had a penchant for dragons  dragonnettes  and dracs running rampant in the streets of Paris and the French countryside alongside the musketeers  that is   Pevel succeeds', 'And by the time I got to the conclusions  I didn t have each story fresh in my mind', ' Yeah  the way people tend to drop dead in your vicinity  and it s never your fault', 'And since I m certain it ll be miles better than a lot of other thrillers out there ', 'Every time I reread an Ellen Hopkins novel I notice something new about the formatting that I didn t notice before  or maybe I did notice but just didn t mention it  ', 'A must read ', 'GHOST by Mike Worley is part of the Angela Masters Detective Series and it is my second read for Mr', 'These writing qualities may echo her very experience but I found it hard to stay connected to her horrific tale ', 'Just when I thought I knew where the story was going  it veered off into another place entirely', 'I don t mind if they know I ve had help', 'But slowly these two develop a bond that is beautiful and lasting and quite magical', 'Hill has poured so much care and attention into this book  and I guess it doesn t really matter who it is or should be dedicated to', 'You can almost see the cracks', 'This was another author I was unfamiliar with', 'Magee and his dog  Dee  go on a camping trip', 'This took me a while to get into  a few hours in the audiobook  because it took that long for a crime to be committed  and I was anxious for a nice bloody murder', 'As part of the blog tour with Lori s Great Escapes Virtual Book Tours  there is an ongoing giveaway', '5 stars  rounded up to 5  because Goodreads has yet to add half stars', 'While I think Hill played the devilish references a bit much  from the Evil Knieval trail to the number of times he throws 666 out there   the book was still a fun read and well worth the price of admission', 'I needed a break from my usual contemporary romances and this thriller kept me glued to my Kindle  Rachel sees a  perfect couple  from the train every day  and she concocts an elaborate fantasy life for them', 'She has brought happiness and goodness to his royally screwed up family and he will do anything to bring her home and win back her love', 'This is very quick read and a very fast paced story', 'There s nothing wrong with the audiobook  it s read by a cast of narrators that would top the all star list of any hipster parent  I m identifying you as a hipster parent if you can immediately recognize the voices of Ira Glass  Sarah Vowell  Rachel Maddow  and Terry Gross ', 'And Gaiman reads this himself  doing a wonderful job  his voice is soothing and pleasant to listen to  and he has a terrific grasp of character voice inflection  of the utmost importance for audiobooks ', 'I also liked how Crawford was able to feed the reader bits and pieces of the background without sounding too matter of fact', 'The characters are interesting and fairly well developed', 'He wants to be fluffy and cute', 'Hoping something will click', ' His facial muscles gave away almost nothing  the throbbing vein in his neck told a different tale', 'Good little read  quirky meets mystery and they work well together', 'The problem Jake has with his hunters is just thrown into the mix', 'But it felt forced in other places and the precociousness got a little too much', 'Okay  this definitely attracted my attention ', 'No man can do that of himself', 'Certain revelations from the first book are largely ignored in this sequel  a flaw that only can be overlooked thanks to stronger writing on Pevel s part', 'Coming off of Paula Hawkins s new release  I can say that very few writers do unsettling  dark humanity and unreliable narrators better than Megan Abbott ', 'Unfortunately  A Cast of Killers does it little justice', 'He s able to take important themes and write about them using well developed and interesting characters', 'I try to say this in a way that does not defer the book', 'Her only mother that she knew dies and her real one can t take her because of stupid reasons', 'August is torn by his actions but he absolutely must ensure his own survival', 'Heroine Elise Benton is witty in the present day just as Elizabeth Bennett was in hers', 'I am glad there will be a part three', 'Sometimes while they were in a lesson  the wolf would jump up randomly and chase the sheep around the field', 'I remained involved']\n"
     ]
    }
   ],
   "source": [
    "extract_sentence = dataset['sentence'].tolist() # converting the pandas series to a python list\n",
    "# Removing all not alphanumeric charecters using the regular expression method \n",
    "# from https://stackoverflow.com/questions/875968/how-to-remove-symbols-from-a-string-with-python\n",
    "extract_sentence = [re.sub(r'[^\\w]', ' ', sent) for sent in extract_sentence]\n",
    "print(extract_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww7Y0PkfJeK1"
   },
   "source": [
    "## TF-IDF\n",
    "Define the TF-IDF vectorizer to count each word in the whole corpus. It generates raw data where the first column represents the position in the matrix of a word in \"sentences\" and the second column represents the frequency of the given word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UcVxAmWSJeK1"
   },
   "outputs": [],
   "source": [
    "#convert to string \n",
    "whole_corpus = [\" \".join(extract_sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtR1P_EJabQt",
    "outputId": "b399bd8b-ed4a-4c1f-e3e5-d62189819885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 719)\t0.0068940929142466715\n",
      "  (0, 209)\t0.0068940929142466715\n",
      "  (0, 1097)\t0.0068940929142466715\n",
      "  (0, 737)\t0.0068940929142466715\n",
      "  (0, 787)\t0.0068940929142466715\n",
      "  (0, 130)\t0.0068940929142466715\n",
      "  (0, 411)\t0.0068940929142466715\n",
      "  (0, 1046)\t0.0068940929142466715\n",
      "  (0, 131)\t0.0068940929142466715\n",
      "  (0, 409)\t0.0068940929142466715\n",
      "  (0, 1321)\t0.0068940929142466715\n",
      "  (0, 1386)\t0.0068940929142466715\n",
      "  (0, 1119)\t0.0068940929142466715\n",
      "  (0, 1304)\t0.0068940929142466715\n",
      "  (0, 353)\t0.0068940929142466715\n",
      "  (0, 319)\t0.0068940929142466715\n",
      "  (0, 1447)\t0.0068940929142466715\n",
      "  (0, 1525)\t0.0068940929142466715\n",
      "  (0, 740)\t0.0068940929142466715\n",
      "  (0, 750)\t0.0068940929142466715\n",
      "  (0, 11)\t0.0068940929142466715\n",
      "  (0, 872)\t0.0068940929142466715\n",
      "  (0, 1440)\t0.0068940929142466715\n",
      "  (0, 1441)\t0.0068940929142466715\n",
      "  (0, 1132)\t0.0068940929142466715\n",
      "  :\t:\n",
      "  (0, 811)\t0.0068940929142466715\n",
      "  (0, 1293)\t0.24818734491288016\n",
      "  (0, 73)\t0.0068940929142466715\n",
      "  (0, 548)\t0.03447046457123336\n",
      "  (0, 923)\t0.013788185828493343\n",
      "  (0, 699)\t0.027576371656986686\n",
      "  (0, 366)\t0.07583502205671339\n",
      "  (0, 249)\t0.0068940929142466715\n",
      "  (0, 725)\t0.027576371656986686\n",
      "  (0, 882)\t0.013788185828493343\n",
      "  (0, 1488)\t0.020682278742740014\n",
      "  (0, 1015)\t0.013788185828493343\n",
      "  (0, 1155)\t0.0068940929142466715\n",
      "  (0, 266)\t0.0068940929142466715\n",
      "  (0, 791)\t0.05515274331397337\n",
      "  (0, 928)\t0.05515274331397337\n",
      "  (0, 655)\t0.013788185828493343\n",
      "  (0, 516)\t0.0068940929142466715\n",
      "  (0, 1197)\t0.020682278742740014\n",
      "  (0, 233)\t0.0068940929142466715\n",
      "  (0, 1256)\t0.0068940929142466715\n",
      "  (0, 177)\t0.0068940929142466715\n",
      "  (0, 1226)\t0.0068940929142466715\n",
      "  (0, 1478)\t0.013788185828493343\n",
      "  (0, 934)\t0.0068940929142466715\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "sentence_tfidf = tfidf_vectorizer.fit_transform(whole_corpus)\n",
    "print(sentence_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-ntrHmYJeK2"
   },
   "source": [
    "Given the raw data we use the function get_feature_names to generate the matrix that represents the TF-IDF (the originality of each word) of each word of the corpus for each sentence(There are 390 sentences and 1543 words). For example the first row and column \"100\" means that the word \"100\" has TF-IDF of zero. The sum of each column represents the TF-IDF that each word appears in the whole corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9ye5qB-JeK2",
    "outputId": "ca30ab92-40a9-4332-ddcc-adf2b0ff2d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        100        12        14        15      1920        20      2013  \\\n",
      "0  0.006894  0.006894  0.013788  0.006894  0.006894  0.006894  0.006894   \n",
      "\n",
      "         50       600       666  ...     wrote        xd      yeah      year  \\\n",
      "0  0.020682  0.006894  0.013788  ...  0.006894  0.013788  0.006894  0.020682   \n",
      "\n",
      "      years    yelled   yelling     young      zero     zusak  \n",
      "0  0.013788  0.013788  0.006894  0.006894  0.006894  0.006894  \n",
      "\n",
      "[1 rows x 1543 columns]\n"
     ]
    }
   ],
   "source": [
    "sentence_tfidf_matrix = pd.DataFrame(sentence_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "print(sentence_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6xiMF6CJeK3"
   },
   "source": [
    "Extract the top 20 TF-IDF keywords per sentnece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOQ12TFHJeK3",
    "outputId": "90655395-4f31-404d-bd04-40fbaab4a3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('book', 0.5308451543969938), ('read', 0.3240223669695936), ('story', 0.24818734491288016), ('just', 0.17924641577041345), ('characters', 0.15856413702767344), ('good', 0.13788185828493343), ('like', 0.13098776537068677), ('really', 0.13098776537068677), ('great', 0.11030548662794674), ('love', 0.11030548662794674), ('novel', 0.11030548662794674), ('character', 0.10341139371370008), ('reading', 0.0965173007994534), ('think', 0.0965173007994534), ('did', 0.08272911497096005), ('world', 0.08272911497096005), ('doesn', 0.07583502205671339), ('interesting', 0.07583502205671339), ('liked', 0.07583502205671339), ('little', 0.07583502205671339)]}\n"
     ]
    }
   ],
   "source": [
    "# convert the concatenated dataframe into a dictionary\n",
    "target_dict_tf_idf = sentence_tfidf_matrix.to_dict('index')\n",
    "# store the top 20 keywords and print them\n",
    "data_repr_tf_idf = {}\n",
    "for sentence_id, target_words in target_dict_tf_idf.items():\n",
    "    list_targets = [(k, v) for k, v in target_words.items()]\n",
    "    list_targets_sorted = sorted(list_targets, key=lambda x: x[1], reverse=True)\n",
    "    data_repr_tf_idf[sentence_id] = list_targets_sorted[0:20]\n",
    "\n",
    "print(data_repr_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ZW07N3QAJeK3"
   },
   "outputs": [],
   "source": [
    "dataframe_keywords = pd.DataFrame.from_dict(data_repr_tf_idf,orient='index', columns=[f'keyword{i}' for i in range(1,21)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "djxVGGkjJeK4",
    "outputId": "9a1a86eb-ee5d-4f9d-e9d2-be78e7c0c9a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword1</th>\n",
       "      <th>keyword2</th>\n",
       "      <th>keyword3</th>\n",
       "      <th>keyword4</th>\n",
       "      <th>keyword5</th>\n",
       "      <th>keyword6</th>\n",
       "      <th>keyword7</th>\n",
       "      <th>keyword8</th>\n",
       "      <th>keyword9</th>\n",
       "      <th>keyword10</th>\n",
       "      <th>keyword11</th>\n",
       "      <th>keyword12</th>\n",
       "      <th>keyword13</th>\n",
       "      <th>keyword14</th>\n",
       "      <th>keyword15</th>\n",
       "      <th>keyword16</th>\n",
       "      <th>keyword17</th>\n",
       "      <th>keyword18</th>\n",
       "      <th>keyword19</th>\n",
       "      <th>keyword20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(book, 0.5308451543969938)</td>\n",
       "      <td>(read, 0.3240223669695936)</td>\n",
       "      <td>(story, 0.24818734491288016)</td>\n",
       "      <td>(just, 0.17924641577041345)</td>\n",
       "      <td>(characters, 0.15856413702767344)</td>\n",
       "      <td>(good, 0.13788185828493343)</td>\n",
       "      <td>(like, 0.13098776537068677)</td>\n",
       "      <td>(really, 0.13098776537068677)</td>\n",
       "      <td>(great, 0.11030548662794674)</td>\n",
       "      <td>(love, 0.11030548662794674)</td>\n",
       "      <td>(novel, 0.11030548662794674)</td>\n",
       "      <td>(character, 0.10341139371370008)</td>\n",
       "      <td>(reading, 0.0965173007994534)</td>\n",
       "      <td>(think, 0.0965173007994534)</td>\n",
       "      <td>(did, 0.08272911497096005)</td>\n",
       "      <td>(world, 0.08272911497096005)</td>\n",
       "      <td>(doesn, 0.07583502205671339)</td>\n",
       "      <td>(interesting, 0.07583502205671339)</td>\n",
       "      <td>(liked, 0.07583502205671339)</td>\n",
       "      <td>(little, 0.07583502205671339)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     keyword1                    keyword2  \\\n",
       "0  (book, 0.5308451543969938)  (read, 0.3240223669695936)   \n",
       "\n",
       "                       keyword3                     keyword4  \\\n",
       "0  (story, 0.24818734491288016)  (just, 0.17924641577041345)   \n",
       "\n",
       "                            keyword5                     keyword6  \\\n",
       "0  (characters, 0.15856413702767344)  (good, 0.13788185828493343)   \n",
       "\n",
       "                      keyword7                       keyword8  \\\n",
       "0  (like, 0.13098776537068677)  (really, 0.13098776537068677)   \n",
       "\n",
       "                       keyword9                    keyword10  \\\n",
       "0  (great, 0.11030548662794674)  (love, 0.11030548662794674)   \n",
       "\n",
       "                      keyword11                         keyword12  \\\n",
       "0  (novel, 0.11030548662794674)  (character, 0.10341139371370008)   \n",
       "\n",
       "                       keyword13                    keyword14  \\\n",
       "0  (reading, 0.0965173007994534)  (think, 0.0965173007994534)   \n",
       "\n",
       "                    keyword15                     keyword16  \\\n",
       "0  (did, 0.08272911497096005)  (world, 0.08272911497096005)   \n",
       "\n",
       "                      keyword17                           keyword18  \\\n",
       "0  (doesn, 0.07583502205671339)  (interesting, 0.07583502205671339)   \n",
       "\n",
       "                      keyword19                      keyword20  \n",
       "0  (liked, 0.07583502205671339)  (little, 0.07583502205671339)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhy3q1nvabQt"
   },
   "source": [
    "## Topic Modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a4TUm4PDNKKp",
    "outputId": "3f63e875-432f-456f-f35d-607c04dde611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\raya\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: future in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: gensim in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (60.9.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: numexpr in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: pip in c:\\users\\raya\\anaconda3\\lib\\site-packages (22.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raya\\anaconda3\\lib\\site-packages (60.9.3)\n",
      "Requirement already satisfied: wheel in c:\\users\\raya\\anaconda3\\lib\\site-packages (0.37.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\raya\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (60.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy) (0.7.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "     --------------------------------------- 13.9/13.9 MB 11.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (60.9.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\raya\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis\n",
    "\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oo_MFVKiJeK4",
    "outputId": "f18f8002-5c59-4092-97e3-ae3f57436051"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Raya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import everygrams\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "l7o8Q6eiOTHA"
   },
   "outputs": [],
   "source": [
    "# Preprocessing step that tokenizes all the sentences\n",
    "# from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6XlWKcjJeK5",
    "outputId": "ad10b731-f089-4c9f-86c6-77b40755e4b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['did', 'like', 'steven', 'or', 'stephen', 'listened', 'to', 'the', 'book'], ['the', 'plot', 'is', 'quick', 'moving', 'and', 'the', 'action', 'is', 'violent'], ['loved', 'everything', 'about', 'this', 'book'], ['great', 'quick', 'read'], ['although', 'there', 'isn', 'character', 'development', 'as', 'is', 'the', 'case', 'with', 'most', 'mystery', 'novels', 'yet', 'the', 'reader', 'comes', 'out', 'enlightened', 'about', 'several', 'notions', 'and', 'ideas', 'in', 'life'], ['liked', 'the', 'ending'], ['believe', 'barnes', 'delivers', 'on', 'that', 'promise', 'in', 'this', 'book', 'the', 'garden', 'of', 'stones', 'was', 'well', 'worth', 'the', 'read'], ['and', 'if', 'you', 'finish', 'pig', 'island', 'and', 'really', 'like', 'the', 'ending', 'read', 'hanging', 'hill'], ['the', 'narrator', 'is', 'good', 'and', 'the', 'audiobook', 'is', 'only', 'about', 'seven', 'hours'], ['great', 'story', 'of', 'girl', 'friendship', 'with', 'her', 'dog'], ['have', 'read', 'all', 'her', 'books', 'and', 'admire', 'her'], ['my', 'year', 'old', 'loved', 'it'], ['very', 'much', 'liked', 'the', 'main', 'characters', 'and', 'never', 'questioned', 'how', 'they', 'acted', 'and', 'reacted', 'in', 'such', 'bizarre', 'situation', 'which', 'is', 'impressive'], ['think', 'that', 'another', 'reason', 'enjoyed', 'it', 'so', 'much', 'was', 'that', 'it', 'reminded', 'me', 'bit', 'of'], ['if', 'you', 'read', 'hayder', 'hanging', 'hill', 'and', 'enjoyed', 'the', 'ending', 'and', 'maybe', 'yelled', 'at', 'the', 'book', 'at', 'little', 'bit', 'then', 'read', 'pig', 'island'], ['cathy', 'ace', 'expertly', 'weaves', 'in', 'comic', 'relief', 'also', 'from', 'the', 'first', 'few', 'pages', 'to', 'relieve', 'the', 'tension', 'she', 'built', 'on', 'the', 'first', 'page', 'of', 'the', 'novel'], ['all', 'in', 'all', 'good', 'first', 'novel', 'and', 'am', 'planning', 'on', 'reading', 'the', 'follow', 'up', 'as', 'the', 'story', 'is', 'pretty', 'solid', 'and', 'the', 'author', 'kept', 'it', 'intriguing', 'enough', 'in', 'general', 'to', 'enjoy', 'the', 'adventure'], ['love', 'you', 'so', 'much'], ['it', 'also', 'the', 'first', 'novel', 'have', 'read', 'fully', 'on', 'my', 'ipad', 'which', 'is', 'saying', 'something', 'as', 'much', 'prefer', 'the', 'feel', 'of', 'book', 'in', 'my', 'hand', 'and', 'all', 'other', 'novels', 'on', 'my', 'ipad', 'have', 'remained', 'unread'], ['barnes', 'presents', 'in', 'the', 'garden', 'of', 'stones', 'well', 'developed', 'fully', 'laid', 'out', 'world'], ['spell', 'binding', 'pageturner'], ['clever', 'story', 'witty', 'and', 'with', 'likeable', 'main', 'character'], ['through', 'series', 'of', 'twists', 'and', 'turns', 'august', 'creates', 'situation', 'where', 'nicole', 'will', 'become', 'his', 'blood', 'mate', 'but', 'she', 'doesn', 'give', 'in', 'so', 'easily', 'and', 'is', 'very', 'strong', 'and', 'feisty', 'heroine'], ['it', 'fits', 'her', 'pretty', 'and', 'carefree', 'as', 'she', 'is'], ['also', 'the', 'audiobook', 'has', 'really', 'nice', 'afterword', 'by', 'hill', 'himself', 'where', 'he', 'talks', 'about', 'the', 'writing', 'of', 'this', 'book', 'and', 'about', 'audiobooks', 'and', 'he', 'gives', 'some', 'audio', 'recommendations'], ['as', 'with', 'any', 'trilogy', 'this', 'book', 'was', 'more', 'about', 'the', 'getting', 'from', 'point', 'book', 'to', 'point', 'book', 'presumably', 'where', 'there', 'is', 'resolution', 'full', 'of', 'nice', 'bits', 'of', 'action', 'and', 'the', 'building', 'up', 'of', 'tension', 'for', 'the', 'third', 'book'], ['like', 'many', 'good', 'fantasies', 'abercrombie', 'doesn', 'try', 'and', 'hide', 'the', 'grit', 'behind', 'feats', 'of', 'magic', 'and', 'enchanted', 'swords'], ['part', 'leopard', 'part', 'rat', 'fink', 'rucpard', 'speaks', 'in', 'third', 'person', 'and', 'is', 'absolutely', 'adorable', 'despite', 'being', 'kilos', 'in', 'weight', 'and', 'very', 'defensive', 'of', 'his', 'charge', 'hero'], ['this', 'is', 'solid', 'police', 'procedural', 'and', 'good', 'start', 'to', 'series'], ['great', 'nonfiction', 'for', 'older', 'kid', 'story', 'times'], ['the', 'kavach', 'building', 'is', 'certainly', 'mystery', 'to', 'be', 'solved', 'but', 'had', 'no', 'clue', 'as', 'to', 'what', 'was', 'waiting', 'for', 'me', 'on', 'the', 'other', 'side', 'of', 'room'], ['great', 'little', 'book', 'all', 'around'], ['and', 'if', 'you', 're', 'reading', 'it', 'alone', 'just', 'as', 'an', 'adult', 'who', 'loves', 'snicket', 'stories', 'then', 'you', 'would', 'have', 'the', 'fun', 'of', 'the', 'illustrations', 'and', 'listening', 'to', 'terry', 'gross', 'read', 'story', 'about', 'kidnapped', 'newt'], ['will', 'admit', 'that', 'enjoyed', 'how', 'she', 'ended', 'this', 'book', 'and', 'loved', 'how', 'she', 'made', 'readers', 'think', 'hard', 'about', 'sebastian', 'and', 'his', 'truthful', 'plans'], ['and', 'love', 'dex'], ['hooked', 'can', 'wait', 'to', 'read', 'rest', 'of', 'series'], ['totally', 'did', 'not', 'see', 'it', 'coming', 'did', 'however', 'predict', 'the', 'ending', 'even', 'before', 'reading', 'the', 'book'], ['the', 'narrator', 'ray', 'porter', 'was', 'great'], ['two', 'of', 'the', 'three', 'main', 'characters', 'are', 'girls', 'and', 'there', 'not', 'vampire', 'fang', 'dragon', 'wing', 'or', 'magic', 'potion', 'in', 'sight', 'which', 'is', 'so', 'refreshing'], ['felt', 'it', 'was', 'bit', 'like', 'nicely', 'written', 'history', 'book'], ['the', 'story', 'merges', 'post', 'apocalyptic', 'future', 'with', 'caribbean', 'voodoo', 'context', 'and', 'the', 'characters', 'are', 'multi', 'generational', 'and', 'well', 'written'], ['she', 'has', 'strong', 'sense', 'of', 'what', 'is', 'right', 'and', 'what', 'is', 'wrong', 'even', 'if', 'that', 'means', 'that', 'her', 'actions', 'will', 'hurt', 'her'], ['once', 'again', 'won', 'soon', 'forget', 'the', 'characters', 'created', 'by', 'emma', 'scott'], ['scanned', 'review', 'that', 'was', 'likening', 'it', 'to', 'lost', 'and', 'while', 'loved', 'the', 'first', 'couple', 'seasons', 'of', 'that', 'show', 'think', 'it', 'would', 'be', 'disservice', 'to', 'compare', 'to', 'something', 'that', 'wasn', 'well', 'thought', 'through'], ['rich', 'girl', 'poor', 'girl', 'russian', 'folklore', 'really', 'liked', 'it'], ['was', 'constantly', 'wondering', 'who', 'would', 'die', 'next', 'and', 'who', 'was', 'committing', 'the', 'murders'], ['horns', 'fit', 'the', 'bill', 'and', 'glad', 'read', 'it'], ['became', 'fan', 'of', 'her', 'when', 'read', 'her', 'story', 'in', 'foretold', 'and', 'excited', 'to', 'read', 'more', 'books', 'by', 'her'], ['loved', 'this', 'book'], ['mystery', 'that', 'produces', 'more', 'mysteries', 'and', 'keeps', 'the', 'reader', 'mind', 'working'], ['could', 'literally', 'stare', 'at', 'your', 'lips', 'for', 'days', 'and', 'never', 'get', 'bored'], ['stars'], ['beautiful', 'illustrations', 'and', 'good', 'mix', 'of', 'technical', 'detail', 'with', 'interesting', 'bits', 'about', 'the', 'lives', 'of', 'the', 'wright', 'brothers', 'the', 'two', 'middle', 'brothers', 'of', 'five', 'siblings'], ['once', 'again', 'colleen', 'hoover', 'love', 'stories', 'leave', 'me', 'speechless', 'and', 'emotional'], ['the', 'novel', 'was', 'fairly', 'paced'], ['awesome'], ['but', 'they', 'were', 'just', 'so', 'perfect', 'and', 'wonderful', 'and', 'marvelous', 'that', 'it', 'was', 'all', 'little', 'tooth', 'rotting', 'really'], ['this', 'book', 'is', 'also', 'fascinating', 'glimpse', 'into', 'the', 'lonely', 'life', 'of', 'writer', 'and', 'the', 'almost', 'obsessive', 'need', 'to', 'get', 'the', 'words', 'out', 'at', 'all', 'costs'], ['this', 'was', 'cool', 'story', 'loved', 'the', 'main', 'character', 'jack'], ['but', 'it', 'so', 'much', 'more', 'than', 'that', 'because', 'of', 'the', 'way', 'lindqvist', 'treats', 'his', 'characters', 'and', 'the', 'way', 'he', 'navigates', 'the', 'narrative'], ['highly', 'recommend'], ['it', 'seems', 'to', 'me', 'that', 'changes', 'has', 'left', 'it', 'mark', 'on', 'the', 'series', 'and', 'this', 'is', 'the', 'end', 'of', 'harry', 'dresden', 'chicago', 'wizard', 'investigator', 'and', 'the', 'start', 'of', 'new', 'series', 'harry', 'dresden', 'superpowered', 'wizard'], ['pressed', 'to', 'death', 'by', 'kirsten', 'weiss', 'starts', 'with', 'the', 'above', 'lines', 'the', 'cozy', 'possibly', 'paranormal', 'mystery', 'novel', 'is', 'exciting', 'quick', 'paced', 'and', 'fun'], ['while', 'often', 'picky', 'about', 'the', 'sci', 'fi', 'books', 'read', 'hero', 'was', 'definitely', 'up', 'my', 'alley'], ['simply', 'stunning', 'emma', 'scott', 'writes', 'the', 'most', 'lyrical', 'and', 'gorgeous', 'fiction'], ['exciting', 'hero', 'by', 'belinda', 'crawford', 'is', 'quick', 'paced', 'and', 'to', 'the', 'point'], ['this', 'trilogy', 'certainly', 'doesn', 'get', 'the', 'popular', 'attention', 'it', 'deserves'], ['it', 'helps', 'that', 'webster', 'reads', 'little', 'like', 'raylan', 'givens', 'that', 'arliss', 'howard', 'has', 'low', 'melodic', 'softly', 'twangy', 'voice', 'and', 'that', 'elmore', 'leonard', 'always', 'writes', 'fast', 'fun', 'story'], ['really', 'wanted', 'it', 'to', 'be', 'book', 'instead', 'of', 'this', 'separate', 'tales'], ['enjoyed', 'the', 'story', 'very', 'much'], ['the', 'direction', 'it', 'took', 'the', 'mature', 'content', 'and', 'themes', 'the', 'emphasis', 'on', 'the', 'psychological', 'and', 'physical', 'torture', 'that', 'was', 'happening', 'that', 'one', 'thing', 'adrian', 'and', 'sydney', 'get', 'married', 'holy', 'crap', 'literally', 'yelled', 'what', 'out', 'loud', 'when', 'read', 'this', 'part'], ['what', 'begins', 'as', 'one', 'night', 'stand', 'with', 'zero', 'expectations', 'for', 'the', 'future', 'turns', 'into', 'burning', 'hot', 'love', 'and', 'deep', 'unbreakable', 'connection'], ['really', 'liked', 'this', 'story'], ['another', 'excellent', 'story', 'from'], ['only', 'kitty', 'thomas', 'could', 'create', 'cast', 'of', 'strong', 'characters', 'like', 'those', 'in', 'blood', 'lust', 'and', 'an', 'fantastically', 'creative', 'story', 'that', 'will', 'keep', 'you', 'guessing', 'until', 'the', 'very', 'end'], ['boys', 'and', 'girls', 'are', 'sure', 'to', 'enjoy'], ['never', 'in', 'million', 'years', 'did', 'think', 'would', 'love', 'book', 'about', 'parallel', 'world', 'with', 'elves', 'kings', 'magic'], ['the', 'reader', 'is', 'catapulted', 'into', 'the', 'conflict', 'head', 'first', 'and', 'although', 'those', 'first', 'few', 'chapters', 'are', 'confusing', 'chaotic', 'scenes', 'of', 'fighting', 'between', 'factions', 'in', 'war', 'we', 'don', 'understand', 'it', 'all', 'makes', 'sense', 'soon', 'enough'], ['hoover', 'has', 'actually', 'made', 'me', 'fall', 'more', 'in', 'love', 'with', 'holder', 'than', 'already', 'was'], ['and', 'it', 'be', 'worth', 'it', 'to', 'stay', 'up', 'past', 'your', 'bedtime', 'to', 'get', 'it', 'finished'], ['an', 'excellent', 'and', 'terrifying', 'book'], ['narrated', 'from', 'the', 'first', 'person', 'perspective', 'of', 'cait', 'morgan', 'the', 'novel', 'is', 'fun', 'mind', 'provoking', 'cozy', 'mystery'], ['but', 'on', 'the', 'whole', 'it', 'fast', 'read', 'and', 'good', 'slice', 'of', 'brain', 'candy', 'thriller'], ['must', 'have', 'gone', 'through', 'box', 'of', 'tissues', 'reading', 'this'], ['but', 'hill', 'take', 'on', 'and', 'exploration', 'of', 'the', 'devil', 'and', 'evil', 'is', 'original', 'and', 'interesting', 'so', 'was', 'okay', 'with', 'some', 'of', 'my', 'minor', 'issues'], ['twisting', 'kingdoms', 'together', 'in', 'this', 'great', 'adventure', 'due', 'to', 'be', 'released', 'in', 'december', 'and', 'definite', 'must', 'for', 'all', 'lovers', 'of', 'medieval', 'but', 'also', 'just', 'good', 'page', 'turner'], ['fun', 'and', 'exciting', 'adventure', 'featuring', 'all', 'of', 'your', 'favourite', 'princes', 'charming', 'and', 'their', 'princesses'], ['if', 'you', 'like', 'lots', 'of', 'high', 'school', 'drama', 'then', 'this', 'book', 'is', 'for', 'you', 'broken', 'prince', 'reminded', 'me', 'of', 'gossip', 'girl', 'in', 'many', 'ways'], ['it', 'is', 'very', 'cute', 'take', 'on', 'jane', 'austen', 'pride', 'and', 'prejudice', 'set', 'amid', 'the', 'privileged', 'offspring', 'of', 'hollywood', 'royalty'], ['so', 'cute', 'sophie', 'doesn', 'want', 'to', 'eat', 'the', 'squash', 'bought', 'at', 'the', 'market', 'but', 'to', 'keep', 'it', 'as', 'her', 'friend', 'and', 'companion', 'all', 'is', 'well', 'until', 'little', 'squash', 'starts', 'getting', 'some', 'brown', 'spots', 'sweet', 'and', 'fun'], ['love', 'the', 'way', 'she', 'makes', 'you', 'feel', 'you', 'truly', 'are', 'down', 'south'], ['must', 'check', 'it', 'out', 'now'], ['but', 'this', 'lovely', 'book', 'and', 'my', 'constant', 'search', 'for', 'new', 'reading', 'material', 'for', 'my', 'children', 'sparked', 'my', 'curiosity'], ['in', 'particularly', 'kind', 'of', 'enjoyed', 'the', 'fact', 'that', 'the', 'devil', 'can', 'be', 'superhero', 'in', 'the', 'right', 'light'], ['names', 'species', 'factions', 'how', 'will', 'you', 'keep', 'it', 'all', 'straight', 'while', 'barnes', 'world', 'is', 'fully', 'realized', 'it', 'is', 'departure', 'from', 'the', 'familiar'], ['the', 'girl', 'exuded', 'calm', 'that', 'danced', 'along', 'my', 'skin', 'and', 'when', 'inhaled', 'it', 'was', 'like', 'breathing', 'in', 'the', 'scent', 'of', 'something', 'delicious', 'but', 'far', 'away'], ['you', 'will', 'not', 'be', 'disappointed'], ['as', 'read', 'the', 'first', 'five', 'or', 'so', 'pages', 'thought', 'to', 'myself', 'lois', 'duncan', 'isn', 'really', 'great', 'writer', 'but', 'then', 'as', 'got', 'about', 'five', 'more', 'pages', 'in', 'realized', 'that', 'no', 'duncan', 'isn', 'great', 'writer', 'but', 'she', 'can', 'tell', 'one', 'hell', 'of', 'story'], ['they', 're', 'happy', 'can', 'tell'], ['while', 'slow', 'to', 'start', 'beginnings', 'although', 'not', 'exposition', 'full', 'are', 'little', 'awkward', 'slow', 'to', 'build', 'sanderson', 'provides', 'stunningly', 'well', 'thought', 'out', 'magic', 'system', 'something', 'he', 'known', 'for', 'and', 'for', 'good', 'reason', 'as', 'well', 'as', 'high', 'paced', 'well', 'written', 'fantasy', 'action'], ['if', 'anything', 'this', 'book', 'is', 'quick', 'enjoyable', 'read', 'that', 'will', 'keep', 'you', 'on', 'the', 'edge', 'of', 'your', 'seat'], ['really', 'enjoyed', 'it'], ['it', 'fairy', 'tale', 'enough', 'for', 'kids', 'but', 'bloody', 'enough', 'for', 'us', 'fans'], ['amazing', 'higginson', 'is', 'now', 'one', 'of', 'my', 'most', 'admired', 'authors'], ['as', 'reader', 'you', 'have', 'this', 'perspective', 'of', 'watching', 'jorg', 'take', 'bite', 'after', 'painful', 'bite', 'from', 'his', 'heart', 'because', 'it', 'is', 'bitter', 'and', 'it', 'is', 'his'], ['gorgeous', 'stars'], ['even', 'though', 'rachel', 'is', 'completely', 'unlikable', 'was', 'cheering', 'for', 'her', 'throughout', 'the', 'book'], ['while', 'think', 'hill', 'played', 'the', 'devilish', 'references', 'bit', 'much', 'from', 'the', 'evil', 'knieval', 'trail', 'to', 'the', 'number', 'of', 'times', 'he', 'throws', 'out', 'there', 'the', 'book', 'was', 'still', 'fun', 'read', 'and', 'well', 'worth', 'the', 'price', 'of', 'admission'], ['the', 'alchemist', 'in', 'the', 'shadows', 'is', 'good', 'read', 'but', 'you', 'may', 'have', 'to'], ['but', 'he', 'was', 'actually', 'perfect', 'and', 'did', 'great', 'voices', 'for', 'variety', 'of', 'characters', 'some', 'with', 'accents'], ['plan', 'to', 'recommend', 'it', 'to', 'my', 'creative', 'writing', 'students', 'who', 'love', 'this', 'genre', 'but', 'don', 'have', 'much', 'experience', 'drawing', 'characters', 'and', 'integrating', 'conflict', 'and', 'the', 'world', 'they', 've', 'created'], ['the', 'suspense', 'that', 'it', 'kept', 'you', 'in', 'was', 'great'], ['once', 'started', 'could', 'not', 'put', 'it', 'down'], ['from', 'the', 'prologue', 'immediately', 'fell', 'in', 'love', 'with', 'ms'], ['all', 'of', 'the', 'ingredients', 'for', 'great', 'read'], ['it', 'fun', 'fantastical', 'mystery', 'full', 'of', 'oddball', 'characters', 'bearded', 'lady', 'time', 'traveling', 'ugly', 'man', 'who', 'may', 'or', 'may', 'not', 'have', 'founded', 'london', 'two', 'assassins', 'dressed', 'like', 'overgrown', 'prep', 'school', 'drop', 'outs', 'human', 'fly', 'and', 'of', 'course', 'the', 'titular', 'side', 'kick', 'the', 'somnambulist'], ['think', 'this', 'could', 'make', 'good', 'read', 'aloud', 'grade'], ['markus', 'zusak', 'made', 'post', 'about', 'how', 'he', 'enjoyed', 'this', 'book'], ['thank', 'you', 'to', 'both', 'parties', 'for', 'the', 'opportunity', 'to', 'read', 'this', 'book', 'in', 'advance', 'it', 'been', 'hard', 'holding', 'this', 'review', 'until', 'closer', 'to', 'publication'], ['ve', 'read', 'it', 'about', 'times', 'to', 'said', 'year', 'old', 'who', 'loves', 'to', 'pick', 'her', 'nose'], ['buy', 'it'], ['she', 'makes', 'it', 'through', 'really', 'tough', 'begin', 'hopefully', 'everything', 'works', 'for', 'her', 'in', 'the', 'next', 'book'], ['time', 'passed', 'and', 'the', 'world', 'turned', 'as', 'they', 'say', 'and', 'gave', 'the', 'book', 'another', 'shot'], ['in', 'ghost', 'we', 'see', 'lot', 'of', 'character', 'background', 'which', 'helps', 'the', 'reader', 'form', 'better', 'picture', 'of', 'each', 'character'], ['would', 'definitely', 'like', 'to', 'read', 'more', 'about', 'the', 'characters', 'in', 'this', 'story'], ['this', 'book', 'is', 'an', 'excellent', 'example', 'of', 'how', 'you', 'can', 'do', 'really', 'good', 'third', 'person', 'mystery'], ['well', 'liked', 'it'], ['it', 'feels', 'good', 'too'], ['it', 'dark', 'and', 'every', 'page', 'drips', 'with', 'what', 'not', 'being', 'said', 'and', 'the', 'tension', 'of', 'knowing', 'that', 'someone', 'probably', 'everyone', 'is', 'lying', 'is', 'thick', 'thick', 'thick'], ['the', 'characters', 'are', 'well', 'developed', 'especially', 'hero', 'who', 'soon', 'realises', 'there', 'is', 'so', 'much', 'more', 'to', 'her', 'being', 'special'], ['wanted', 'to', 'be', 'maggie', 'and', 'think', 'of', 'the', 'book', 'often'], ['what', 'this', 'book', 'did', 'extremely', 'well', 'was', 'create', 'tension', 'and', 'suspense'], ['although', 'really', 'liked', 'how', 'demille', 'character', 'grew', 'from', 'the', 'previous', 'novel', 'he', 'appeared', 'in', 'it', 'took', 'me', 'awhile', 'to', 'really', 'fall', 'into', 'the', 'book', 'and', 'become', 'engaged', 'in', 'the', 'story'], ['after', 'coming', 'home', 're', 'read', 'it'], ['loved', 'this', 'book'], ['needed', 'something', 'that', 'was', 'fun', 'to', 'read', 'knew', 'not', 'to', 'take', 'itself', 'too', 'seriously', 'and', 'was', 'outside', 'my', 'usual', 'reading', 'circles', 'without', 'being', 'just', 'foreign'], ['alessandra', 'torre', 'deserves', 'all', 'the', 'accolades', 'for', 'this', 'one'], ['there', 'were', 'lot', 'of', 'absolutely', 'unbelievable', 'plot', 'points', 'but', 'didn', 'mind', 'so', 'much'], ['there', 'were', 'lot', 'of', 'beautiful', 'lines', 'and', 'images', 'in', 'the', 'novel'], ['this', 'was', 'well', 'written', 'easy', 'to', 'read', 'debut', 'novel'], ['he', 'then', 'leaves', 'her', 'alone', 'in', 'the', 'middle', 'of', 'the', 'woods', 'and', 'so', 'begins', 'this', 'epic', 'tale', 'that', 'had', 'me', 'swooning', 'at', 'every', 'word'], ['one', 'sitting', 'tears', 'awesome'], ['it', 'his', 'ride', 'and', 'loved', 'being', 'on', 'it'], ['there', 'are', 'many', 'great', 'twists', 'and', 'turns'], ['felt', 'his', 'pain', 'and', 'the', 'pressure', 'he', 'was', 'under', 'to', 'try', 'and', 'protect', 'the', 'innocent', 'ella', 'at', 'all', 'costs'], ['certainly', 'read', 'it'], ['she', 'is', 'pure', 'americana', 'rough', 'and', 'raspy', 'and', 'great', 'hero', 'and', 'even', 'better', 'villain'], ['guess', 'that', 'could', 'be', 'interpreted', 'as', 'good', 'way', 'to', 'bring', 'the', 'reader', 'back', 'for', 'the', 'following', 'books'], ['the', 'writing', 'duo', 'that', 'makes', 'up', 'erin', 'watt', 'are', 'incredibly', 'talented', 'and', 'love', 'their', 'other', 'books'], ['thought', 'it', 'was', 'going', 'to', 'go', 'off', 'in', 'direction', 'wouldn', 'enjoy', 'when', 'discovered', 'the', 'antagonist', 'but', 'was', 'pleasantly', 'surprised'], ['the', 'best', 'book', 'in', 'the', 'bloodlines', 'series', 'so', 'far', 'in', 'my', 'opinion'], ['many', 'lambasted', 'this', 'novel', 'as', 'dark', 'and', 'misogynistic', 'but', 'have', 'to', 'admit', 'that', 'didn', 'see', 'any', 'of', 'that', 'when', 'read', 'it'], ['his', 'descriptions', 'and', 'use', 'of', 'novelistic', 'elements', 'to', 'tell', 'this', 'story', 'work', 'beautifully', 'and', 'bring', 'out', 'more', 'dimensions', 'than', 'another', 'writer', 'would', 'have', 'gotten', 'to'], ['looking', 'forward', 'to', 'the', 'rest'], ['and', 'please', 'don', 'be', 'put', 'off', 'by', 'this', 'being', 'the', 'second', 'book', 'in', 'series'], ['this', 'is', 'tale', 'of', 'revenge', 'as', 'rich', 'in', 'its', 'single', 'mindedness', 'as', 'any', 'classic', 'by', 'dumas'], ['in', 'particularly', 'kind', 'of', 'enjoyed', 'the', 'fact', 'that', 'the', 'devil', 'can', 'be', 'superhero', 'in', 'the', 'right', 'light'], ['but', 'if', 'you', 'are', 'looking', 'for', 'summer', 'pool', 'or', 'beach', 'read', 'then', 'pick', 'up', 'broken', 'prince', 'and', 'enjoy', 'the', 'crazy', 'ride'], ['some', 'sass', 'and', 'strength', 'and', 'lot', 'of', 'humanity', 'in', 'non', 'human'], ['and', 'when', 'they', 'meet', 'it', 'is', 'pure', 'magic'], ['crawford', 'has', 'done', 'tremendous', 'job', 'crafting', 'her', 'world', 'building', 'and', 'setting'], ['ve', 'tried', 'to', 'read', 'few', 'sanderson', 'books', 'but', 'this', 'is', 'the', 'first', 've', 'actually', 'followed', 'through', 'all', 'the', 'way'], ['looking', 'forward', 'to', 'cornell', 'next', 'book', 'about', 'this', 'group'], ['and', 'since', 'certain', 'it', 'll', 'be', 'miles', 'better', 'than', 'lot', 'of', 'other', 'thrillers', 'out', 'there'], ['every', 'time', 'reread', 'an', 'ellen', 'hopkins', 'novel', 'notice', 'something', 'new', 'about', 'the', 'formatting', 'that', 'didn', 'notice', 'before', 'or', 'maybe', 'did', 'notice', 'but', 'just', 'didn', 'mention', 'it'], ['must', 'read'], ['but', 'slowly', 'these', 'two', 'develop', 'bond', 'that', 'is', 'beautiful', 'and', 'lasting', 'and', 'quite', 'magical'], ['hill', 'has', 'poured', 'so', 'much', 'care', 'and', 'attention', 'into', 'this', 'book', 'and', 'guess', 'it', 'doesn', 'really', 'matter', 'who', 'it', 'is', 'or', 'should', 'be', 'dedicated', 'to'], ['this', 'took', 'me', 'while', 'to', 'get', 'into', 'few', 'hours', 'in', 'the', 'audiobook', 'because', 'it', 'took', 'that', 'long', 'for', 'crime', 'to', 'be', 'committed', 'and', 'was', 'anxious', 'for', 'nice', 'bloody', 'murder'], ['stars', 'rounded', 'up', 'to', 'because', 'goodreads', 'has', 'yet', 'to', 'add', 'half', 'stars'], ['while', 'think', 'hill', 'played', 'the', 'devilish', 'references', 'bit', 'much', 'from', 'the', 'evil', 'knieval', 'trail', 'to', 'the', 'number', 'of', 'times', 'he', 'throws', 'out', 'there', 'the', 'book', 'was', 'still', 'fun', 'read', 'and', 'well', 'worth', 'the', 'price', 'of', 'admission'], ['needed', 'break', 'from', 'my', 'usual', 'contemporary', 'romances', 'and', 'this', 'thriller', 'kept', 'me', 'glued', 'to', 'my', 'kindle', 'rachel', 'sees', 'perfect', 'couple', 'from', 'the', 'train', 'every', 'day', 'and', 'she', 'concocts', 'an', 'elaborate', 'fantasy', 'life', 'for', 'them'], ['and', 'gaiman', 'reads', 'this', 'himself', 'doing', 'wonderful', 'job', 'his', 'voice', 'is', 'soothing', 'and', 'pleasant', 'to', 'listen', 'to', 'and', 'he', 'has', 'terrific', 'grasp', 'of', 'character', 'voice', 'inflection', 'of', 'the', 'utmost', 'importance', 'for', 'audiobooks'], ['also', 'liked', 'how', 'crawford', 'was', 'able', 'to', 'feed', 'the', 'reader', 'bits', 'and', 'pieces', 'of', 'the', 'background', 'without', 'sounding', 'too', 'matter', 'of', 'fact'], ['the', 'characters', 'are', 'interesting', 'and', 'fairly', 'well', 'developed'], ['good', 'little', 'read', 'quirky', 'meets', 'mystery', 'and', 'they', 'work', 'well', 'together'], ['coming', 'off', 'of', 'paula', 'hawkins', 'new', 'release', 'can', 'say', 'that', 'very', 'few', 'writers', 'do', 'unsettling', 'dark', 'humanity', 'and', 'unreliable', 'narrators', 'better', 'than', 'megan', 'abbott'], ['he', 'able', 'to', 'take', 'important', 'themes', 'and', 'write', 'about', 'them', 'using', 'well', 'developed', 'and', 'interesting', 'characters'], ['am', 'glad', 'there', 'will', 'be', 'part', 'three'], ['remained', 'involved']]\n"
     ]
    }
   ],
   "source": [
    "extract_sentence_positive = dataset.loc[dataset['score']==1]['sentence'].tolist()\n",
    "# Removing all not alphanumeric charecters using the regular expression method \n",
    "# from https://stackoverflow.com/questions/875968/how-to-remove-symbols-from-a-string-with-python\n",
    "extract_sentence_positive = [re.sub(r'[^\\w]', ' ', sent) for sent in extract_sentence_positive]\n",
    "extract_sentence_positive = list(sent_to_words(extract_sentence_positive))\n",
    "print(extract_sentence_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCoiLv-5JeK5",
    "outputId": "6253db03-83db-4805-e8c0-5291a4b7c4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['did', 'like', 'steven', 'or', 'stephen', 'listened', 'to', 'the', 'book']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram models for positive \n",
    "#code from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
    "bigram = gensim.models.Phrases(extract_sentence_positive, min_count=5, threshold=100) \n",
    "\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "\n",
    "print(bigram_mod[extract_sentence_positive[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "bnA4MRcZJeK6"
   },
   "outputs": [],
   "source": [
    "#code from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djZlLYBUJeK6",
    "outputId": "d99eb0da-450c-4a1f-9913-b02f7221e8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like', 'steven', 'stephen', 'listened', 'book'], ['plot', 'quick', 'moving', 'action', 'violent'], ['loved', 'everything', 'book'], ['great', 'quick', 'read'], ['although', 'character', 'development', 'case', 'mystery', 'novels', 'yet', 'reader', 'comes', 'enlightened', 'several', 'notions', 'ideas', 'life']]\n",
      "[['listen', 'book'], ['plot', 'quick', 'move', 'action', 'violent'], ['love', 'book'], ['great', 'quick', 'read'], ['character', 'development', 'case', 'mystery', 'novel', 'reader', 'come', 'enlighten', 'several', 'notion', 'idea', 'life']]\n"
     ]
    }
   ],
   "source": [
    "#code from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
    "stop_words = stopwords.words('english')\n",
    "data_words_nostops_pos = remove_stopwords(extract_sentence_positive)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops_pos)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_pos = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_words_bigrams[:5])\n",
    "print(data_lemmatized_pos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jcqkJNV8abQu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "id2word_pos = corpora.Dictionary(data_lemmatized_pos)\n",
    "texts_pos = data_lemmatized_pos\n",
    "corpus_pos = [id2word_pos.doc2bow(text) for text in texts_pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aL0VX0aNabQv",
    "outputId": "cd6865ad-f5f2-497e-a31b-6019f0d032a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.045*\"love\" + 0.039*\"story\" + 0.024*\"make\" + 0.020*\"voice\" + 0.016*\"time\" '\n",
      "  '+ 0.013*\"meet\" + 0.013*\"actually\" + 0.013*\"girl\" + 0.012*\"begin\" + '\n",
      "  '0.010*\"ride\"'),\n",
      " (1,\n",
      "  '0.061*\"read\" + 0.040*\"book\" + 0.025*\"fun\" + 0.022*\"great\" + 0.015*\"look\" + '\n",
      "  '0.014*\"thriller\" + 0.012*\"hero\" + 0.012*\"many\" + 0.011*\"go\" + 0.011*\"try\"'),\n",
      " (2,\n",
      "  '0.054*\"character\" + 0.021*\"like\" + 0.019*\"get\" + 0.018*\"way\" + '\n",
      "  '0.016*\"writer\" + 0.016*\"star\" + 0.013*\"fact\" + 0.011*\"next\" + 0.011*\"thick\" '\n",
      "  '+ 0.011*\"use\"'),\n",
      " (3,\n",
      "  '0.050*\"book\" + 0.048*\"well\" + 0.029*\"read\" + 0.025*\"really\" + 0.023*\"think\" '\n",
      "  '+ 0.022*\"enjoy\" + 0.018*\"much\" + 0.017*\"develop\" + 0.016*\"mystery\" + '\n",
      "  '0.016*\"reader\"'),\n",
      " (4,\n",
      "  '0.034*\"good\" + 0.030*\"take\" + 0.025*\"novel\" + 0.017*\"keep\" + 0.016*\"notice\" '\n",
      "  '+ 0.014*\"new\" + 0.014*\"interesting\" + 0.013*\"first\" + 0.013*\"dark\" + '\n",
      "  '0.013*\"audiobook\"')]\n"
     ]
    }
   ],
   "source": [
    "lda_model_pos = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus_pos, id2word=id2word_pos,num_topics=5,  random_state=42, #num_topics is 5 because this is the most amount with non-overlapping topics \n",
    "   update_every=1, chunksize=20, passes=10, alpha='auto', per_word_topics=True\n",
    ")\n",
    "pprint(lda_model_pos.print_topics())\n",
    "doc_lda = lda_model_pos[corpus_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fneOQsuBJeK7",
    "outputId": "df5202b6-48f9-4c2b-a75e-32290a37195d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.006104014728797\n",
      "\n",
      "Coherence Score:  0.5663075204304808\n"
     ]
    }
   ],
   "source": [
    "#Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_pos.log_perplexity(corpus_pos))  # a measure of how good the topic model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score to judge how good a topic model is\n",
    "coherence_model_lda_pos = CoherenceModel(model=lda_model_pos, texts=data_lemmatized_pos, dictionary=id2word_pos, coherence='c_v')\n",
    "coherence_lda_pos = coherence_model_lda_pos.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "CGeqizfiabQw",
    "outputId": "728c2d6a-658c-4cf6-df54-f9cd564f5100"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2201216925606089449804944220\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2201216925606089449804944220_data = {\"mdsDat\": {\"x\": [0.18770658269586726, -0.1309299607263737, 0.08313098150202933, -0.0774699640856976, -0.06243763938582565], \"y\": [0.05768939334870867, 0.1600760024243434, -0.03890169718731514, -0.1460968299303728, -0.03276686865536426], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [26.62108296976441, 21.719087159936308, 19.48628605688186, 17.20437355153675, 14.96917026188067]}, \"tinfo\": {\"Term\": [\"read\", \"character\", \"well\", \"book\", \"love\", \"story\", \"good\", \"take\", \"really\", \"novel\", \"fun\", \"think\", \"make\", \"enjoy\", \"like\", \"great\", \"voice\", \"way\", \"develop\", \"keep\", \"get\", \"notice\", \"writer\", \"star\", \"much\", \"mystery\", \"reader\", \"bit\", \"look\", \"time\", \"well\", \"really\", \"think\", \"enjoy\", \"develop\", \"reader\", \"bit\", \"mystery\", \"world\", \"say\", \"little\", \"work\", \"matter\", \"feel\", \"worth\", \"come\", \"evil\", \"able\", \"perfect\", \"admission\", \"devilish\", \"knieval\", \"number\", \"play\", \"price\", \"reference\", \"still\", \"throw\", \"trail\", \"life\", \"much\", \"book\", \"lot\", \"read\", \"write\", \"tale\", \"good\", \"take\", \"novel\", \"notice\", \"keep\", \"new\", \"interesting\", \"dark\", \"guess\", \"start\", \"follow\", \"page\", \"beautiful\", \"audiobook\", \"know\", \"bloody\", \"set\", \"release\", \"enough\", \"sanderson\", \"together\", \"theme\", \"put\", \"fairly\", \"hour\", \"maybe\", \"narrator\", \"reading\", \"ellen\", \"format\", \"first\", \"write\", \"fun\", \"look\", \"thriller\", \"many\", \"hero\", \"go\", \"series\", \"forward\", \"turn\", \"try\", \"even\", \"part\", \"see\", \"usual\", \"fantasy\", \"prince\", \"twist\", \"quick\", \"certainly\", \"point\", \"broken\", \"person\", \"building\", \"great\", \"suspense\", \"glad\", \"direction\", \"group\", \"rest\", \"break\", \"concoct\", \"attention\", \"read\", \"book\", \"character\", \"like\", \"way\", \"star\", \"writer\", \"fact\", \"thick\", \"use\", \"humanity\", \"create\", \"next\", \"background\", \"tell\", \"tension\", \"need\", \"right\", \"bring\", \"human\", \"kind\", \"light\", \"particularly\", \"superhero\", \"commit\", \"murder\", \"soon\", \"add\", \"goodread\", \"half\", \"round\", \"awesome\", \"get\", \"day\", \"devil\", \"lot\", \"much\", \"love\", \"story\", \"make\", \"voice\", \"girl\", \"meet\", \"actually\", \"begin\", \"fall\", \"listen\", \"leave\", \"excellent\", \"pick\", \"far\", \"grasp\", \"importance\", \"inflection\", \"pleasant\", \"soothe\", \"terrific\", \"utmost\", \"middle\", \"alone\", \"time\", \"ride\", \"year\", \"admit\", \"epic\", \"swoon\", \"wood\", \"magic\", \"job\", \"great\"], \"Freq\": [27.0, 13.0, 17.0, 28.0, 9.0, 8.0, 10.0, 9.0, 9.0, 7.0, 7.0, 8.0, 5.0, 8.0, 5.0, 7.0, 4.0, 4.0, 6.0, 5.0, 6.0, 5.0, 4.0, 4.0, 8.0, 6.0, 6.0, 6.0, 4.0, 4.0, 17.440390829535787, 9.180784440789171, 8.400177125680065, 7.8696849253358465, 6.295800020949538, 5.664883617817662, 5.535946588012692, 5.740090408762637, 4.77908644847132, 4.7174737918342595, 4.253933527230863, 4.396754364898149, 3.4691103011923783, 3.457796883522422, 3.2336770264280617, 3.26548324998896, 3.1516375058474027, 3.4194179972916374, 2.878750550746148, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.5757166867997956, 2.6781369792486145, 6.545300799000913, 18.021636797269867, 4.620863792398797, 10.397554673444992, 3.9345632187072352, 2.603096893583785, 10.163725497002543, 8.845101091739272, 7.3398827205573, 4.792418559235228, 5.082537437182851, 4.205514341171751, 4.1009117937628945, 3.776478216531726, 3.265360092720783, 3.0240156069195696, 3.0652501197355058, 3.0644428539045356, 2.9319469526217605, 3.7698150482057136, 2.50291796463988, 2.38631046028328, 2.2325265939703076, 2.2295775072352026, 2.160878800738046, 2.2117410771222223, 2.2062225189679703, 2.0978213480178693, 2.024438660639179, 1.9955787013168553, 1.8748239751173004, 1.8736254282791736, 1.8688389265585144, 1.7923598902487596, 1.68371209286237, 1.68371209286237, 3.8987316409417865, 2.4686935715305567, 6.480131821677439, 3.944056735140808, 3.6304701135400017, 3.210846633310045, 3.2201435194436923, 2.971097742680787, 2.9208579620442854, 2.7895327653705877, 2.810440723725612, 2.9486666129995305, 2.525446133514771, 2.5190867393098713, 2.7032552683194937, 2.503459764935402, 2.390798034544411, 2.330371353033275, 2.047507355664617, 1.9937594787399688, 1.9353284348750939, 1.9076505525494738, 1.8100608302300152, 1.783495343196326, 2.0324603438565627, 5.880649513037548, 1.6973122901555164, 1.631992497717797, 1.6730590190607113, 1.6203116083467053, 1.528827007896765, 1.6066887678853938, 1.6066887678853938, 1.897804856828032, 16.234952457468463, 10.4639944291686, 12.559668475000034, 4.961063607534234, 4.097320695364206, 3.8065476257772692, 3.8187115122105624, 3.048624698461269, 2.571889018102709, 2.553497727991882, 2.5514347551175955, 2.452189409151919, 2.5737891361411145, 2.304405712054592, 2.1533777465911994, 2.142274699602852, 2.4864482829916335, 1.9598000958768977, 2.193246088936786, 1.8166203636959473, 1.671397662171953, 1.671397662171953, 1.671397662171953, 1.671397662171953, 1.7493420739496839, 1.7493420739496839, 1.5962605938034078, 1.5035561851245318, 1.5035561851245318, 1.5035561851245318, 1.5035561851245318, 1.4740582604355148, 4.329635619190111, 1.6269867787966876, 1.6716319426601534, 1.790074699302784, 1.8300456684608353, 9.15824257950974, 7.968871708398481, 4.973842699541722, 4.105744439687305, 2.5662733547970893, 2.681239455315528, 2.621302348054997, 2.50468204815177, 2.0462788807587216, 1.9991809260196236, 1.9440319919554538, 1.8910370736958584, 1.8982009829254047, 1.7457119497930136, 1.5734069550001688, 1.5734069550001688, 1.5734069550001688, 1.5734069550001688, 1.5734069550001688, 1.5734069550001688, 1.5734069550001688, 1.5461040574119156, 1.4836280454346016, 3.3078618736641308, 2.125781284745484, 1.3679797579329118, 1.4067331243070038, 1.2426188190686067, 1.2426188190686067, 1.2426188190686067, 1.8778059379716256, 1.4823628969458356, 1.3825634438059944], \"Total\": [27.0, 13.0, 17.0, 28.0, 9.0, 8.0, 10.0, 9.0, 9.0, 7.0, 7.0, 8.0, 5.0, 8.0, 5.0, 7.0, 4.0, 4.0, 6.0, 5.0, 6.0, 5.0, 4.0, 4.0, 8.0, 6.0, 6.0, 6.0, 4.0, 4.0, 17.968534537879236, 9.706259435344972, 8.920804178616184, 8.410984385528403, 6.858268374146679, 6.1867390113445335, 6.055644969869009, 6.283767064153069, 5.321663787799557, 5.270778925390376, 4.775551986149746, 4.945467252718824, 3.996876126572243, 3.992313251997787, 3.7496295179996526, 3.7922141428017873, 3.6776106150188856, 3.9915489581394095, 3.4340072155575747, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.091938977291615, 3.2273850354118387, 8.771963003382188, 28.877517837849904, 6.8234190246514785, 27.03317757971185, 6.800723505591579, 3.154514204636683, 10.70838463225494, 9.383723761575974, 7.875452346612861, 5.320966319731309, 5.658636951390036, 4.734085741816363, 4.637478599338615, 4.3288912609686, 3.831921831204811, 3.551559788485752, 3.601910778755206, 3.602326349462196, 3.4591095914743217, 4.44849037733189, 3.048088302267915, 2.9229304141403265, 2.7605341112770976, 2.7604479840829756, 2.6910426524148288, 2.760170270993983, 2.7617902021006016, 2.6348732848047747, 2.565475258218814, 2.538338345302801, 2.405747939465312, 2.406694649103136, 2.4063323955642755, 2.3197057632490883, 2.2139211337136713, 2.2139211337136713, 5.297507832942624, 6.800723505591579, 7.06750805505123, 4.477242199903145, 4.1973323655405705, 3.7438069930273357, 3.762144459558514, 3.5029688417026925, 3.4568587263051183, 3.320457792113156, 3.355749475331952, 3.5558808164638482, 3.0576435253418577, 3.0522893770848047, 3.293099267086757, 3.0546433011096235, 2.9334964020490455, 2.8742372408226373, 2.58726617786174, 2.531062525968594, 2.4655908088191563, 2.4404333763788473, 2.346094920762223, 2.321435602834034, 2.6485307021178497, 7.672932453072182, 2.2374107899801476, 2.1632098472485466, 2.225126458463803, 2.1621937143857775, 2.0590723459177442, 2.1640854520323693, 2.1640854520323693, 2.5842785117274545, 27.03317757971185, 28.877517837849904, 13.1207827297194, 5.5103233079918175, 4.660747866716088, 4.348117871832933, 4.372072982436775, 3.5976141466137372, 3.122707344844322, 3.112287865666601, 3.1124321396546564, 2.9976833984388382, 3.157668292369971, 2.868783825012393, 2.699426914191559, 2.695638514937026, 3.1649505762279757, 2.5009974538596724, 2.7992042093064162, 2.3580517328420347, 2.2128509711029833, 2.2128509711029833, 2.2128509711029833, 2.2128509711029833, 2.3379810562642747, 2.3379810562642747, 2.1448135242818247, 2.0444746775768836, 2.0444746775768836, 2.0444746775768836, 2.0444746775768836, 2.014744691124268, 6.1017074692939115, 2.3485319214401996, 2.75878771807486, 6.8234190246514785, 8.771963003382188, 9.695577975212856, 8.51584778880009, 5.520231546997017, 4.6419510369965336, 3.1046549926426934, 3.2481501366535883, 3.18282456447027, 3.046025403164859, 2.587659001559672, 2.5355617545942257, 2.4845888156097033, 2.4349884000385447, 2.452072693489317, 2.298554998595269, 2.121517431821424, 2.121517431821424, 2.121517431821424, 2.121517431821424, 2.121517431821424, 2.121517431821424, 2.121517431821424, 2.0899328598196574, 2.0194977518174033, 4.520925917574914, 2.916005154614864, 1.902808341138251, 2.0216566905446527, 1.7859811359885005, 1.7859811359885005, 1.7859811359885005, 3.1842677486955413, 3.6803739457979945, 7.672932453072182], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.0308, -3.6725, -3.7613, -3.8265, -4.0497, -4.1553, -4.1783, -4.1421, -4.3253, -4.3383, -4.4417, -4.4087, -4.6457, -4.6489, -4.7159, -4.7062, -4.7416, -4.6601, -4.8322, -4.9434, -4.9434, -4.9434, -4.9434, -4.9434, -4.9434, -4.9434, -4.9434, -4.9434, -4.9434, -4.9044, -4.0108, -2.998, -4.359, -3.548, -4.5198, -4.9329, -3.3672, -3.5062, -3.6927, -4.119, -4.0602, -4.2497, -4.2748, -4.3573, -4.5027, -4.5795, -4.5659, -4.5662, -4.6104, -4.359, -4.7686, -4.8163, -4.8829, -4.8842, -4.9155, -4.8923, -4.8948, -4.9452, -4.9808, -4.9951, -5.0575, -5.0582, -5.0607, -5.1025, -5.1651, -5.1651, -4.3254, -4.7824, -3.7088, -4.2054, -4.2882, -4.411, -4.4081, -4.4886, -4.5057, -4.5517, -4.5442, -4.4962, -4.6512, -4.6537, -4.5831, -4.6599, -4.7059, -4.7315, -4.8609, -4.8876, -4.9173, -4.9317, -4.9842, -4.999, -4.8683, -3.8059, -5.0485, -5.0878, -5.0629, -5.095, -5.1531, -5.1034, -5.1034, -4.9369, -2.7904, -3.2296, -2.9225, -3.8514, -4.0427, -4.1163, -4.1131, -4.3383, -4.5084, -4.5156, -4.5164, -4.556, -4.5076, -4.6182, -4.686, -4.6912, -4.5422, -4.7802, -4.6676, -4.856, -4.9394, -4.9394, -4.9394, -4.9394, -4.8938, -4.8938, -4.9854, -5.0452, -5.0452, -5.0452, -5.0452, -5.065, -3.9875, -4.9663, -4.9392, -4.8708, -4.8487, -3.0992, -3.2383, -3.7097, -3.9015, -4.3714, -4.3276, -4.3502, -4.3957, -4.5978, -4.6211, -4.6491, -4.6767, -4.6729, -4.7567, -4.8606, -4.8606, -4.8606, -4.8606, -4.8606, -4.8606, -4.8606, -4.8781, -4.9194, -4.1176, -4.5597, -5.0005, -4.9726, -5.0966, -5.0966, -5.0966, -4.6838, -4.9202, -4.9899], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2936, 1.2678, 1.2633, 1.2569, 1.2379, 1.2353, 1.2337, 1.233, 1.2159, 1.2126, 1.2078, 1.2059, 1.1819, 1.1797, 1.1754, 1.1739, 1.1691, 1.1688, 1.1471, 1.1408, 1.1408, 1.1408, 1.1408, 1.1408, 1.1408, 1.1408, 1.1408, 1.1408, 1.1408, 1.1369, 1.0307, 0.852, 0.9337, 0.368, 0.7762, 1.1313, 1.4748, 1.4679, 1.4566, 1.4224, 1.4196, 1.4086, 1.404, 1.3905, 1.367, 1.3662, 1.3656, 1.3653, 1.3616, 1.3614, 1.3299, 1.3241, 1.3147, 1.3134, 1.3076, 1.3055, 1.3024, 1.299, 1.2901, 1.2864, 1.2776, 1.2766, 1.2742, 1.2691, 1.2532, 1.2532, 1.2204, 0.5136, 1.5487, 1.5087, 1.4904, 1.4819, 1.4799, 1.4708, 1.467, 1.4612, 1.4581, 1.4482, 1.4442, 1.4435, 1.4381, 1.4365, 1.4309, 1.4257, 1.4015, 1.3968, 1.3933, 1.3892, 1.3761, 1.3718, 1.3707, 1.3694, 1.3592, 1.3537, 1.3503, 1.347, 1.3377, 1.3376, 1.3376, 1.3267, 1.1256, 0.6203, 1.7163, 1.655, 1.6312, 1.627, 1.6247, 1.5944, 1.5659, 1.5621, 1.5613, 1.5591, 1.5556, 1.5409, 1.534, 1.5302, 1.5187, 1.5162, 1.5161, 1.4991, 1.4794, 1.4794, 1.4794, 1.4794, 1.47, 1.47, 1.4646, 1.4527, 1.4527, 1.4527, 1.4527, 1.4475, 1.4169, 1.3929, 1.259, 0.4219, 0.1928, 1.8422, 1.8328, 1.795, 1.7764, 1.7087, 1.7074, 1.7051, 1.7035, 1.6644, 1.6615, 1.6538, 1.6464, 1.6432, 1.6241, 1.6003, 1.6003, 1.6003, 1.6003, 1.6003, 1.6003, 1.6003, 1.5978, 1.5908, 1.5868, 1.5831, 1.5692, 1.5365, 1.5364, 1.5364, 1.5364, 1.3711, 0.9898, 0.1854]}, \"token.table\": {\"Topic\": [1, 5, 4, 1, 5, 5, 3, 2, 4, 4, 2, 5, 1, 2, 1, 3, 3, 4, 3, 3, 3, 4, 1, 4, 3, 4, 2, 4, 1, 2, 4, 1, 3, 2, 1, 2, 5, 3, 1, 5, 4, 2, 5, 3, 5, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 3, 3, 2, 4, 5, 3, 5, 3, 2, 4, 3, 2, 4, 4, 5, 5, 2, 2, 5, 2, 4, 1, 2, 5, 1, 4, 4, 5, 1, 3, 1, 4, 5, 3, 5, 5, 3, 1, 2, 5, 5, 1, 4, 4, 1, 2, 4, 2, 4, 2, 2, 1, 2, 3, 4, 1, 3, 5, 1, 5, 3, 1, 3, 2, 3, 1, 3, 1, 2, 1, 1, 2, 3, 5, 4, 4, 2, 1, 3, 3, 2, 4, 5, 4, 2, 1, 5, 4, 3, 5, 2, 1, 4, 4, 5, 2, 4, 1, 3, 1, 4, 5, 2, 1, 3, 3, 3, 4, 3, 5, 5, 4, 1, 5, 1, 1, 1, 1, 2, 4, 5], \"Freq\": [0.7515879252545602, 0.9425590192713941, 0.9782464033109987, 0.9702649444355629, 0.494643825866691, 0.49517262353972497, 0.7739103935291808, 0.8991814437506135, 0.49634080407576603, 0.6971595358849878, 0.8672752107635183, 0.9848900133541112, 0.9908110580878039, 0.6842448216777774, 0.6233222710162198, 0.3462901505645666, 0.9241779238069038, 0.7144887798291636, 0.8524804270707939, 0.7551356676366773, 0.8111646072195811, 0.9907945484497797, 0.7910945656100321, 0.8554389243836239, 0.9241779238069038, 0.6671818648498967, 0.9240241343241757, 0.8515958338660912, 0.8748564029103824, 0.36247805275058315, 0.7249561055011663, 0.9702649444355629, 0.8988253195194905, 0.9033745464298287, 0.9511371836291213, 0.7432063546838561, 0.5599163282575896, 0.981147728679257, 0.8157470472127714, 0.8213591489669277, 0.8338859804695168, 0.7879170259949794, 0.7728993653315721, 0.6817802805563359, 0.8701118751660383, 0.7514440402437798, 0.188768007813313, 0.755072031253252, 0.832891258077408, 0.9033745464298287, 0.9034898763434619, 0.8489555233986228, 0.16388855169350158, 0.6555542067740063, 0.9662909428291706, 0.9245520043022464, 0.8564164100705464, 0.9338476664238226, 0.9782464033109987, 0.9427214549365756, 0.7819696102756186, 0.13032826837926975, 0.924986501761313, 0.7828969723677158, 0.9782464033109987, 0.7974175452986323, 0.8313422895187053, 0.8481578127166473, 0.9638764366226049, 0.9427214549365756, 0.9427214549365756, 0.8625376730731371, 0.5434230405536553, 0.27171152027682766, 0.8836050170654185, 0.9038114297426505, 0.9702649444355629, 0.9842234549989464, 0.8049621681602925, 0.9295451168928089, 0.9038114297426505, 0.9073877739166997, 0.7887798419329237, 0.8375995092506512, 0.8934071067423002, 0.7327704750266875, 0.29310819001067495, 0.928258224832895, 0.31404394319845036, 0.6280878863969007, 0.9057591076446746, 0.8013233603087335, 0.7505861840589059, 0.8310152684908771, 0.923602627275953, 0.9569685411676728, 0.797996981667732, 0.22799913761935203, 0.8554389243836239, 0.9548412502793314, 0.8311403710005774, 0.6319214002967537, 0.8449361118806625, 0.9500681269305732, 0.9396789416724748, 0.8888378332973619, 0.9702649444355629, 0.8327951742761676, 0.9828688008819316, 0.9038114297426505, 0.8736149377929873, 0.861535852021214, 0.8156365042970997, 0.9702649444355629, 0.9427214549365756, 0.8195265723531576, 0.9702649444355629, 0.6958367846585896, 0.7795826498785187, 0.7901819806820595, 0.3699158180910596, 0.5918653089456953, 0.9698162455209258, 0.8621783123040168, 0.9272367032790042, 0.9702649444355629, 0.7245200820780554, 0.9713111848474585, 0.6858698438289123, 0.799680942063133, 0.9782464033109987, 0.7245929792873854, 0.9486263929443178, 0.910995921071627, 0.86783991985885, 0.7244974774373449, 0.9324819977856516, 0.9427214549365756, 0.9199382624633896, 0.8446992810668926, 0.9702649444355629, 0.9394249637154712, 0.9038114297426505, 0.8938903883706335, 0.5599163282575896, 0.9591075173006235, 0.9510180666140069, 0.740897999307002, 0.741939243306413, 0.9427214549365756, 0.7590497848735013, 0.9607048207553246, 0.8967801377342843, 0.952986242604794, 0.9702649444355629, 0.2211936267552054, 0.6635808802656162, 0.7241679684716137, 0.9702649444355629, 0.8436728211220968, 0.8939880709370412, 0.7730167143656285, 0.9639211183177137, 0.9821113970689233, 0.9427214549365756, 0.8617066332927343, 0.8582313642334736, 0.9460983011253655, 0.5599163282575896, 0.8088214511583222, 0.9395557854411992, 0.8000790439692389, 0.5881727137871707, 0.29408635689358537, 0.9148978107338456, 0.5255390037873203], \"Term\": [\"able\", \"actually\", \"add\", \"admission\", \"admit\", \"alone\", \"attention\", \"audiobook\", \"awesome\", \"background\", \"beautiful\", \"begin\", \"bit\", \"bloody\", \"book\", \"book\", \"break\", \"bring\", \"broken\", \"building\", \"certainly\", \"character\", \"come\", \"commit\", \"concoct\", \"create\", \"dark\", \"day\", \"develop\", \"devil\", \"devil\", \"devilish\", \"direction\", \"ellen\", \"enjoy\", \"enough\", \"epic\", \"even\", \"evil\", \"excellent\", \"fact\", \"fairly\", \"fall\", \"fantasy\", \"far\", \"feel\", \"first\", \"first\", \"follow\", \"format\", \"forward\", \"fun\", \"get\", \"get\", \"girl\", \"glad\", \"go\", \"good\", \"goodread\", \"grasp\", \"great\", \"great\", \"group\", \"guess\", \"half\", \"hero\", \"hour\", \"human\", \"humanity\", \"importance\", \"inflection\", \"interesting\", \"job\", \"job\", \"keep\", \"kind\", \"knieval\", \"know\", \"leave\", \"life\", \"light\", \"like\", \"listen\", \"little\", \"look\", \"lot\", \"lot\", \"love\", \"magic\", \"magic\", \"make\", \"many\", \"matter\", \"maybe\", \"meet\", \"middle\", \"much\", \"much\", \"murder\", \"mystery\", \"narrator\", \"need\", \"new\", \"next\", \"notice\", \"novel\", \"number\", \"page\", \"part\", \"particularly\", \"perfect\", \"person\", \"pick\", \"play\", \"pleasant\", \"point\", \"price\", \"prince\", \"put\", \"quick\", \"read\", \"read\", \"reader\", \"reading\", \"really\", \"reference\", \"release\", \"rest\", \"ride\", \"right\", \"round\", \"sanderson\", \"say\", \"see\", \"series\", \"set\", \"soon\", \"soothe\", \"star\", \"start\", \"still\", \"story\", \"superhero\", \"suspense\", \"swoon\", \"take\", \"tale\", \"tell\", \"tension\", \"terrific\", \"theme\", \"thick\", \"think\", \"thriller\", \"throw\", \"time\", \"time\", \"together\", \"trail\", \"try\", \"turn\", \"twist\", \"use\", \"usual\", \"utmost\", \"voice\", \"way\", \"well\", \"wood\", \"work\", \"world\", \"worth\", \"write\", \"write\", \"writer\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 5, 2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2201216925606089449804944220\", ldavis_el2201216925606089449804944220_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2201216925606089449804944220\", ldavis_el2201216925606089449804944220_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2201216925606089449804944220\", ldavis_el2201216925606089449804944220_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3      0.187707  0.057689       1        1  26.621083\n",
       "4     -0.130930  0.160076       2        1  21.719087\n",
       "1      0.083131 -0.038902       3        1  19.486286\n",
       "2     -0.077470 -0.146097       4        1  17.204374\n",
       "0     -0.062438 -0.032767       5        1  14.969170, topic_info=          Term       Freq      Total Category  logprob  loglift\n",
       "9         read  27.000000  27.000000  Default  30.0000  30.0000\n",
       "11   character  13.000000  13.000000  Default  29.0000  29.0000\n",
       "30        well  17.000000  17.000000  Default  28.0000  28.0000\n",
       "0         book  28.000000  28.000000  Default  27.0000  27.0000\n",
       "7         love   9.000000   9.000000  Default  26.0000  26.0000\n",
       "..         ...        ...        ...      ...      ...      ...\n",
       "582      swoon   1.242619   1.785981   Topic5  -5.0966   1.5364\n",
       "583       wood   1.242619   1.785981   Topic5  -5.0966   1.5364\n",
       "141      magic   1.877806   3.184268   Topic5  -4.6838   1.3711\n",
       "631        job   1.482363   3.680374   Topic5  -4.9202   0.9898\n",
       "8        great   1.382563   7.672932   Topic5  -4.9899   0.1854\n",
       "\n",
       "[200 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "673       1  0.751588       able\n",
       "369       5  0.942559   actually\n",
       "654       4  0.978246        add\n",
       "492       1  0.970265  admission\n",
       "178       5  0.494644      admit\n",
       "...     ...       ...        ...\n",
       "31        1  0.800079      worth\n",
       "127       1  0.588173      write\n",
       "127       2  0.294086      write\n",
       "274       4  0.914898     writer\n",
       "47        5  0.525539       year\n",
       "\n",
       "[176 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 5, 2, 3, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_pos = pyLDAvis.gensim_models.prepare(lda_model_pos, corpus_pos, id2word_pos)\n",
    "vis_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUyKzG2HJeK8"
   },
   "source": [
    "## Topic Modelling Negative Sentiments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGAuLwsrJeK8",
    "outputId": "f67e2163-30f0-41ed-f2fa-0014125a0bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mild', 'because', 'it', 'isn', 'conclusive', 'and', 'doesn', 'give', 'us', 'the', 'information', 'we', 'need', 'to', 'fully', 'appreciate', 'the', 'story', 'that', 'lloyd', 'is', 'telling'], ['going', 'in', 'really', 'liked', 'it', 'but', 'unfortunately', 'left', 'me', 'bored', 'and', 'infuriated', 'at', 'the', 'end', 'which', 'think', 'is', 'really', 'hard', 'to', 'do', 'this', 'is', 'the', 'first', 'book', 'finished', 'only', 'because', 'wanted', 'it', 'over', 'with'], ['not', 'giving', 'this', 'stars', 'because', 'the', 'big', 'reveal', 'kind', 'of', 'disappointed', 'me', 'in', 'that', 'it', 'classic', 'case', 'of', 'possible', 'but', 'not', 'really', 'probable'], ['eh', 'hate', 'how', 'the', 'author', 'made', 'duke', 'from', 'nice', 'guy', 'to', 'complete', 'dick', 'wad', 'and', 'trevor', 'into', 'mr'], ['guess', 'didn', 'track', 'this', 'on', 'goodreads', 'when', 'first', 'read', 'this', 'so', 'll', 'just', 'rate', 'it', 'stars', 'but', 'have', 'to', 'reread', 'it'], ['but', 'it', 'would', 'have', 'been', 'so', 'much', 'more', 'interesting', 'if', 'hill', 'had', 'gone', 'with', 'the', 'more', 'simple', 'logical', 'and', 'real', 'explanation', 'lee', 'doesn', 'react', 'to', 'the', 'horns', 'because', 'lee', 'has', 'no', 'guilt'], ['love', 'anything', 'that', 'incorporates', 'shakespeare', 'and', 'that', 'was', 'clever', 'but', 'the', 'main', 'issue', 'that', 'turned', 'me', 'off', 'is', 'the', 'step', 'focus'], ['but', 'would', 'not', 'be', 'honest', 'if', 'said', 'enjoyed', 'this', 'read'], ['found', 'it', 'very', 'difficult', 'to', 'buy', 'into', 'the', 'world', 'the', 'author', 'was', 'creating', 'when', 'first', 'started', 'reading', 'this', 'book'], ['this', 'is', 'the', 'first', 'book', 'completed', 'for', 'my', 'goodreads', 'reading', 'challenge', 'and', 'can', 'say', 'that', 'the', 'bar', 'has', 'been', 'set', 'so', 'low', 'that', 'it', 'can', 'only', 'get', 'higher'], ['ve', 'liked', 'several', 'of', 'his', 'other', 'novels', 'better'], ['just', 'the', 'journey', 'took', 'up', 'most', 'of', 'the', 'book', 'and', 'the', 'final', 'confrontation', 'was', 'kind', 'of', 'short'], ['readers', 'of', 'the', 'electronic', 'edition', 'though', 'would', 'suffer', 'the', 'entire', 'length', 'of', 'the', 'book', 'before', 'discovering', 'that', 'hidden', 'away', 'at', 'the', 'back', 'there', 'is', 'mild', 'attempt', 'at', 'explaining', 'who', 'who'], ['ve', 'read', 'wayyy', 'too', 'many', 'books', 'where', 'the', 'author', 'turns', 'one', 'of', 'the', 'guys', 'into', 'dick', 'head', 'so', 'it', 'gives', 'the', 'main', 'character', 'an', 'excuse', 'to', 'fall', 'in', 'love', 'with', 'the', 'right', 'guy'], ['just', 'because', 'you', 're', 'monster', 'doesn', 'mean', 'you', 'can', 'do', 'some', 'good'], ['there', 'was', 'no', 'sexual', 'tension', 'among', 'any', 'of', 'the', 'main', 'characters'], ['hate'], ['felt', 'the', 'beginning', 'was', 'kinda', 'slow'], ['assume', 'it', 'was', 'meant', 'to', 'be', 'funny', 'but', 'it', 'seemed', 'low', 'brow', 'and', 'childish', 'and', 'detracted', 'from', 'an', 'otherwise', 'solid', 'and', 'affecting', 'story'], ['maybe', 'it', 'just', 'me', 'but', 'was', 'annoyed', 'at', 'how', 'ethan', 'had', 'to', 'mention', 'his', 'undying', 'unyielding', 'eternal', 'love', 'for', 'lena', 'practically', 'every'], ['but', 'he', 'not', 'perfect', 'writer', 'and', 'neither', 'is', 'his', 'son'], ['but', 'lucas', 'now', 'just', 'wanna', 'punch', 'your', 'face', 'out'], ['was', 'pretty', 'disappointed'], ['give', 'it', 'like'], ['photos', 'are', 'mismarked', 'the', 'research', 'is', 'poor'], ['it', 'made', 'hazel', 'come', 'off', 'as', 'prig', 'which', 'sure', 'wasn', 'the', 'intention', 'and', 'might', 'have', 'unfairly', 'colored', 'my', 'view', 'of', 'her'], ['only', 'not', 'that', 'good'], ['those', 'last', 'like', 'pages', 'were', 'full', 'of', 'twists', 'and', 'turns', 'like', 'still', 'not', 'sure', 'what', 'just', 'happened'], ['my', 'biggest', 'beef', 'is', 'with', 'the', 'reason', 'lee', 'tourneau', 'is', 'impervious', 'to', 'ig', 'horns', 'at', 'the', 'start'], ['the', 'concept', 'of', 'this', 'book', 'was', 'great', 'idea', 'but', 'was', 'disappointed', 'by', 'the', 'book', 'in', 'general'], ['dont', 'get', 'the', 'obsession', 'some', 'people', 'have', 'with', 'this', 'book', 'xd', 'nothing', 'interesting', 'appened', 'till', 'the', 'last', 'pages', 'and', 'then', 'it', 'ended', 'before', 'barely', 'knew', 'anything', 'xd'], ['archie', 'unbearably', 'creepy', 'obsession', 'with', 'gretchen', 'lowell', 'just', 'gets', 'creepier', 'and', 'it', 'makes', 'for', 'irritatingly', 'good', 'reading'], ['the', 'characters', 'inhabit', 'sparse', 'semi', 'arid', 'landscape', 'that', 'is', 'nearly', 'devoid', 'of', 'other', 'human', 'life'], ['however', 'the', 'over', 'the', 'top', 'cruelty', 'of', 'reed', 'and', 'ella', 'fellow', 'students', 'here', 'turned', 'me', 'off'], ['but', 'feel', 'it', 'almost', 'unfortunate', 'for', 'those', 'few', 'authors', 'who', 'have', 'talent', 'and', 'didn', 'just', 'spew', 'out', 'filth', 'to', 'be', 'included', 'with', 'the', 'rest'], ['too', 'much', 'information', 'especially', 'when', 'revealed', 'in', 'story', 'utilizing', 'the', 'third', 'person', 'omniscience', 'of', 'the', 'all', 'knowing', 'all', 'seeing', 'all', 'explaining', 'author', 'lends', 'detachment', 'to', 'the', 'reader', 'that', 'makes', 'it', 'difficult', 'to', 'invest', 'fully', 'in', 'the', 'characters'], ['not', 'so', 'much', 'that', 'it', 'was', 'supposedly', 'cliffhanger', 'because', 'to', 'me', 'it', 'honestly', 'wasn', 'but', 'just', 'the', 'ending', 'itself'], ['so', 'it', 'wasn', 'good', 'enough', 'to', 'get', 'published', 'on', 'its', 'own', 'he', 'had', 'to', 'publish', 'something', 'better', 'presumably', 'and', 'build', 'name', 'for', 'himself'], ['agree', 'with', 'the', 'other', 'reviewer', 'who', 'said', 'it', 'had', 'far', 'too', 'much', 'going', 'on'], ['she', 'was', 'right', 'that', 'the', 'first', 'pages', 'make', 'it', 'feel', 'as', 'though', 'you', 've', 'jumped', 'into', 'sequel', 'without', 'much', 'of', 'an', 'explanation', 'of', 'who', 'the', 'characters', 'are', 'and', 'what', 'sort', 'of', 'world', 'they', 're', 'living', 'in'], ['first', 'the', 'list', 'of', 'characters', 'in', 'addition', 'to', 'being', 'too', 'long', 'to', 'keep', 'track', 'of', 'in', 'such', 'short', 'book', 'was', 'tucked', 'away', 'at', 'the', 'back', 'of', 'the', 'book'], ['not', 'sure', 'they', 'needed', 'pages', 'to', 'show', 'all', 'the', 'animals', 'laughing', 'in', 'the', 'end'], ['so', 'why', 'the', 'struggle', 'up', 'to', 'three', 'stars', 'this', 'book', 'failed', 'for', 'me', 'on', 'purely', 'technical', 'merits'], ['not', 'the', 'same', 'impact', 'it', 'created', 'on', 'me', 'on', 'the', 'first', 'book'], ['can'], ['to', 'be', 'even', 'more', 'honest', 'did', 'not', 'finish', 'it', 'and', 'merely', 'skimmed'], ['the', 'characters', 'were', 'somewhat', 'stereotypes', 'and', 'did', 'not', 'find', 'myself', 'attached', 'to', 'any', 'of', 'them'], ['it', 'made', 'some', 'of', 'hill', 'cheesier', 'dialogue', 'even', 'cheesier'], ['did', 'however', 'feel', 'that', 'the', 'conclusion', 'to', 'the', 'corpse', 'with', 'the', 'garnet', 'face', 'was', 'rather', 'confusing', 'even', 'as', 'cait', 'laid', 'out', 'all', 'their', 'findings', 'still', 'felt', 'lost'], ['really', 'aggravated', 'and', 'disappointed', 'but', 'happy', 'that', 'finally', 'finished', 'reading', 'and', 'got', 'it', 'over', 'with'], ['the', 'book', 'only', 'got', 'interesting', 'on', 'the', 'part', 'where', 'cricket', 'started', 'appearing'], ['simply', 'dropping', 'in', 'reference', 'to', 'someone', 'cell', 'phone', 'in', 'one', 'scene', 'doesn', 'fix', 'this', 'it', 'only', 'highlights', 'the', 'absence', 'of', 'technology', 'in', 'every', 'other', 'scene'], ['feel', 'like', 'there', 'was', 'lot', 'left', 'unexplained'], ['every', 'time', 'felt', 'had', 'grasp', 'we', 'were', 'introduced', 'to', 'new', 'character', 'that', 'made', 'me', 'question', 'whether', 'we', 'were', 'speaking', 'the', 'same', 'language'], ['or', 'possibly', 'it', 'because', 'wanted', 'more', 'of', 'something', 'more', 'development', 'of', 'oaksey', 'or', 'of', 'his', 'wife', 'lexie', 'or', 'more', 'time', 'spent', 'on', 'the', 'island', 'with', 'the', 'villagers'], ['ohlsson', 'though', 'seems', 'to', 'have', 'fascination', 'with', 'unfaithful', 'husbands', 'they', 're', 'all', 'over', 'the', 'place', 'and', 'each', 'is', 'dealt', 'with', 'in', 'different', 'way', 'which', 'is', 'interesting', 'if', 'done', 'as', 'character', 'study', 'but', 'there', 'not', 'enough', 'detail', 'in', 'each', 'relationship', 'so', 'they', 're', 'just', 'sort', 'of'], ['like', 'okay', 'we', 'get', 'it', 'ethan'], ['and', 'by', 'the', 'time', 'got', 'to', 'the', 'conclusions', 'didn', 'have', 'each', 'story', 'fresh', 'in', 'my', 'mind'], ['these', 'writing', 'qualities', 'may', 'echo', 'her', 'very', 'experience', 'but', 'found', 'it', 'hard', 'to', 'stay', 'connected', 'to', 'her', 'horrific', 'tale'], ['but', 'it', 'felt', 'forced', 'in', 'other', 'places', 'and', 'the', 'precociousness', 'got', 'little', 'too', 'much'], ['certain', 'revelations', 'from', 'the', 'first', 'book', 'are', 'largely', 'ignored', 'in', 'this', 'sequel', 'flaw', 'that', 'only', 'can', 'be', 'overlooked', 'thanks', 'to', 'stronger', 'writing', 'on', 'pevel', 'part'], ['unfortunately', 'cast', 'of', 'killers', 'does', 'it', 'little', 'justice']]\n"
     ]
    }
   ],
   "source": [
    "extract_sentence_neg = dataset.loc[dataset['score']==-1]['sentence'].tolist()\n",
    "# Removing all not alphanumeric charecters using the regular expression method \n",
    "# from https://stackoverflow.com/questions/875968/how-to-remove-symbols-from-a-string-with-python\n",
    "extract_sentence_neg = [re.sub(r'[^\\w]', ' ', sent) for sent in extract_sentence_neg]\n",
    "extract_sentence_neg = list(sent_to_words(extract_sentence_neg))\n",
    "print(extract_sentence_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG1jUfefJeK9",
    "outputId": "5a4650d7-19aa-4193-865c-12f9f4711790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['did', 'like', 'steven', 'or', 'stephen', 'listened', 'to', 'the', 'book']\n"
     ]
    }
   ],
   "source": [
    "#code from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
    "#build trigram models for negative sentiments \n",
    "trigram = gensim.models.Phrases(extract_sentence_neg, threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "print(trigram_mod[bigram_mod[extract_sentence_positive[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "M7MRIbh9Sfpl"
   },
   "outputs": [],
   "source": [
    "# Define the trigram function\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaJLFy_fJeK9",
    "outputId": "d6944b33-4bc7-4e0d-b6f9-0a109d0817b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mild', 'conclusive', 'give', 'us', 'information', 'need', 'fully', 'appreciate', 'story', 'lloyd', 'telling'], ['going', 'really', 'liked', 'unfortunately', 'left', 'bored', 'infuriated', 'end', 'think', 'really', 'hard', 'first', 'book', 'finished', 'wanted'], ['giving', 'stars', 'big', 'reveal', 'kind', 'disappointed', 'classic', 'case', 'possible', 'really', 'probable'], ['eh', 'hate', 'author', 'made', 'duke', 'nice', 'guy', 'complete', 'dick', 'wad', 'trevor', 'mr'], ['guess', 'track', 'goodreads', 'first', 'read', 'rate', 'stars', 'reread']]\n",
      "[['mild', 'conclusive', 'give', 'information', 'need', 'fully', 'appreciate', 'story', 'lloyd', 'tell'], ['go', 'really', 'like', 'unfortunately', 'leave', 'bored', 'infuriated', 'end', 'think', 'really', 'hard', 'first', 'book', 'finish', 'want'], ['give', 'star', 'big', 'reveal', 'kind', 'disappoint', 'classic', 'case', 'possible', 'really', 'probable'], ['hate', 'author', 'make', 'nice', 'guy', 'trevor'], ['guess', 'track', 'goodread', 'first', 'read', 'rate', 'star', 'reread']]\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops_neg = remove_stopwords(extract_sentence_neg)\n",
    "data_words_trigrams = make_trigrams(data_words_nostops_neg)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_neg = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_words_trigrams[:5])\n",
    "print(data_lemmatized_neg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Qud_vGr0JeK9"
   },
   "outputs": [],
   "source": [
    "id2word_neg = corpora.Dictionary(data_words_nostops_neg)\n",
    "texts_neg = data_words_nostops_neg\n",
    "corpus_neg = [id2word_neg.doc2bow(text) for text in texts_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORv2HbmFJeK9",
    "outputId": "98897b6d-ea51-49fe-9cf1-125ac35d670d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.013*\"made\" + 0.012*\"characters\" + 0.011*\"felt\" + 0.010*\"get\" + '\n",
      "  '0.009*\"time\" + 0.009*\"cheesier\" + 0.009*\"good\" + 0.008*\"something\" + '\n",
      "  '0.008*\"like\" + 0.007*\"every\"'),\n",
      " (1,\n",
      "  '0.019*\"book\" + 0.011*\"disappointed\" + 0.009*\"feel\" + 0.009*\"like\" + '\n",
      "  '0.008*\"pages\" + 0.008*\"though\" + 0.008*\"stars\" + 0.008*\"scene\" + '\n",
      "  '0.008*\"even\" + 0.007*\"got\"'),\n",
      " (2,\n",
      "  '0.036*\"book\" + 0.034*\"first\" + 0.030*\"part\" + 0.030*\"little\" + '\n",
      "  '0.029*\"writing\" + 0.029*\"sequel\" + 0.029*\"unfortunately\" + 0.028*\"certain\" '\n",
      "  '+ 0.028*\"flaw\" + 0.028*\"ignored\"')]\n"
     ]
    }
   ],
   "source": [
    "lda_model_neg = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus_neg, id2word=id2word_neg,num_topics=3,  random_state=42, #num_topics is 5 because this is the most amount with non-overlapping topics \n",
    "   update_every=1, chunksize=20, passes=10, alpha='auto', per_word_topics=True\n",
    ")\n",
    "pprint(lda_model_neg.print_topics())\n",
    "doc_lda = lda_model_neg[corpus_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUvdq0vjJeK-",
    "outputId": "4fc42844-c32f-4b51-cc63-3df682f7fd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.676255399802769\n",
      "\n",
      "Coherence Score:  0.5382328175723966\n"
     ]
    }
   ],
   "source": [
    "#Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_neg.log_perplexity(corpus_neg))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score to judge how good the topic model is \n",
    "coherence_model_lda_neg = CoherenceModel(model=lda_model_neg, texts=data_words_nostops_neg, dictionary=id2word_neg, coherence='c_v')\n",
    "coherence_lda_neg = coherence_model_lda_neg.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "jEk5hD2xJeK-",
    "outputId": "e749f8c3-5906-4d92-dd97-02f655172f75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el220121692600345424301132\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el220121692600345424301132_data = {\"mdsDat\": {\"x\": [-0.07698975471798339, -0.0878648722580898, 0.16485462697607317], \"y\": [-0.061315914526385136, 0.05867734575913822, 0.0026385687672469877], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [38.56325352734635, 31.658252484784644, 29.77849398786901]}, \"tinfo\": {\"Term\": [\"part\", \"little\", \"sequel\", \"unfortunately\", \"writing\", \"first\", \"largely\", \"thanks\", \"certain\", \"stronger\", \"revelations\", \"flaw\", \"pevel\", \"overlooked\", \"ignored\", \"killers\", \"cast\", \"justice\", \"book\", \"made\", \"disappointed\", \"characters\", \"cheesier\", \"felt\", \"get\", \"something\", \"good\", \"though\", \"ethan\", \"like\", \"disappointed\", \"though\", \"stars\", \"scene\", \"still\", \"sort\", \"read\", \"one\", \"honest\", \"away\", \"back\", \"track\", \"feel\", \"turns\", \"right\", \"short\", \"pages\", \"absence\", \"cell\", \"dropping\", \"fix\", \"highlights\", \"phone\", \"reference\", \"simply\", \"someone\", \"technology\", \"dealt\", \"detail\", \"different\", \"even\", \"sure\", \"like\", \"book\", \"got\", \"character\", \"really\", \"story\", \"characters\", \"first\", \"would\", \"interesting\", \"made\", \"cheesier\", \"something\", \"ethan\", \"information\", \"fully\", \"better\", \"give\", \"get\", \"good\", \"hate\", \"development\", \"island\", \"lexie\", \"oaksey\", \"possibly\", \"spent\", \"villagers\", \"wife\", \"grasp\", \"introduced\", \"language\", \"new\", \"question\", \"speaking\", \"whether\", \"dialogue\", \"attached\", \"find\", \"somewhat\", \"felt\", \"characters\", \"time\", \"every\", \"like\", \"author\", \"much\", \"story\", \"love\", \"main\", \"part\", \"sequel\", \"unfortunately\", \"little\", \"certain\", \"flaw\", \"ignored\", \"largely\", \"overlooked\", \"pevel\", \"revelations\", \"stronger\", \"thanks\", \"cast\", \"justice\", \"killers\", \"writing\", \"first\", \"book\", \"lee\", \"reading\", \"started\", \"xd\", \"obsession\", \"horns\", \"going\", \"world\", \"explanation\", \"appearing\", \"cricket\", \"much\", \"interesting\", \"got\", \"really\"], \"Freq\": [5.0, 5.0, 5.0, 5.0, 5.0, 7.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 9.0, 2.0, 2.0, 3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 2.2146857215968847, 1.7300189950352787, 1.6059929578615189, 1.5826334573408556, 1.3968476873214961, 1.3713330276890585, 1.286548147016492, 1.263226852691106, 1.2632265617809895, 1.2632260769307957, 1.2632260769307957, 1.263182828293502, 1.9421968095979116, 1.0774224644243018, 1.063302172228081, 1.1903783050014694, 1.7554868181389878, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 0.9204246746662686, 1.5826320997603127, 1.3968476873214961, 1.87323617864292, 3.873804878841749, 1.4992774301896383, 1.26322811330161, 1.1309798895700798, 1.263226755721067, 1.369582330609015, 1.3225166616790676, 0.9437903812694135, 0.9204246746662686, 2.2821050481056173, 1.5037403935168738, 1.3272189665996763, 1.2002581831853172, 1.0237176506064949, 1.0237176506064949, 1.0237174117857246, 1.0237171729649543, 1.6390519431996793, 1.4765609977173229, 0.8967465978990125, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745417591380595, 0.8745413611034424, 0.8745412814965189, 0.8745412814965189, 0.8745412814965189, 1.8249309251460193, 2.1055760585306937, 1.5037407119445676, 1.2002585812199342, 1.3272180113165952, 1.0237174117857246, 1.032466531099045, 1.0237176506064949, 0.8967484288582515, 0.896748349251328, 4.773134608212683, 4.684300707154716, 4.622568323929385, 4.764184035647752, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.461000050841246, 4.460999451800149, 4.460999451800149, 4.460999451800149, 4.730419982723782, 5.450948711974736, 5.790814678893297, 0.6693984000174494, 0.8974105894463863, 0.5954114704915455, 0.5708070554756892, 0.5708060820339051, 0.5078345074173396, 0.5078337586159671, 0.49936357957151856, 0.49936342981124404, 0.43382646169273315, 0.43382646169273315, 1.0986490114033998, 0.8199635610986888, 0.6249654251390959, 0.507180616618865], \"Total\": [5.0, 5.0, 5.0, 5.0, 5.0, 7.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 9.0, 2.0, 2.0, 3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 2.5824578701346175, 2.1016860432717235, 1.9731423411193623, 1.9497761178265345, 1.7639904016038848, 1.7504914803394076, 1.653703210165285, 1.6303694733733232, 1.6303691824632067, 1.630369096642216, 1.630369096642216, 1.6303654844141724, 2.588938531761107, 1.444565085106519, 1.437093813318556, 1.626715635443555, 2.4026528685016015, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 1.2875673152502167, 2.578912726871858, 2.216598107588111, 3.3221965915733653, 9.926403900391549, 2.6926758134999247, 2.2595076729969783, 1.883564311325579, 2.408683251134893, 3.6097305756284492, 7.025900758828894, 1.472472221546519, 1.985792020999861, 2.662122296977113, 1.8837577079084893, 1.707236215471172, 1.5802754414168299, 1.4037349088380073, 1.4037349330805171, 1.40373466065722, 1.4037344218364498, 2.250465370184873, 2.080998304793968, 1.2767638654905422, 1.254559090097101, 1.254559090097101, 1.254559090097101, 1.254559090097101, 1.254559090097101, 1.254559090097101, 1.254559090097101, 1.254559090097101, 1.254559177707123, 1.254559177707123, 1.254559177707123, 1.254559177707123, 1.254559177707123, 1.254559177707123, 1.254559177707123, 1.2545586380549894, 1.2545585303680145, 1.2545585303680145, 1.2545585303680145, 2.8693317676430437, 3.6097305756284492, 2.5459039405080506, 2.242422003723495, 3.3221965915733653, 1.9080132238444278, 2.3989328016708837, 2.408683251134893, 1.619504965161963, 1.6195048761950226, 5.276816806871768, 5.190638668409973, 5.126250602195394, 5.285895490916258, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682269402062, 4.964682092638092, 4.964682092638092, 4.964682092638092, 5.32467787034598, 7.025900758828894, 9.926403900391549, 1.1730805986765347, 1.5747806716284067, 1.0990936691506308, 1.0744892541347746, 1.0744901315539606, 1.011516706076425, 1.0115172906130856, 1.0210041887057795, 1.0210044025831504, 0.9375086603518186, 0.9375086603518186, 2.3989328016708837, 1.985792020999861, 2.6926758134999247, 1.883564311325579], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.5436, -4.7906, -4.865, -4.8796, -5.0045, -5.0229, -5.0867, -5.105, -5.105, -5.105, -5.105, -5.1051, -4.6749, -5.2641, -5.2773, -5.1644, -4.776, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -5.4216, -4.8796, -5.0045, -4.711, -3.9845, -4.9337, -5.105, -5.2156, -5.105, -5.0242, -5.0592, -5.3966, -5.4216, -4.3163, -4.7334, -4.8583, -4.9589, -5.118, -5.118, -5.118, -5.118, -4.6473, -4.7517, -5.2504, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -5.2755, -4.5399, -4.3968, -4.7334, -4.9589, -4.8583, -5.118, -5.1094, -5.118, -5.2504, -5.2504, -3.5172, -3.536, -3.5492, -3.5191, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5848, -3.5262, -3.3844, -3.3239, -5.4816, -5.1884, -5.5987, -5.6409, -5.6409, -5.7578, -5.7578, -5.7746, -5.7746, -5.9153, -5.9153, -4.9861, -5.2787, -5.5502, -5.7591], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7992, 0.7583, 0.747, 0.7442, 0.7195, 0.7088, 0.7018, 0.6977, 0.6977, 0.6977, 0.6977, 0.6977, 0.6654, 0.6596, 0.6516, 0.6406, 0.639, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.6172, 0.4646, 0.4911, 0.3799, 0.0119, 0.3673, 0.3714, 0.4428, 0.3075, -0.0163, -0.7172, 0.5081, 0.1839, 0.9961, 0.9249, 0.8984, 0.8751, 0.8345, 0.8345, 0.8345, 0.8345, 0.8332, 0.807, 0.7969, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.6976, 0.6111, 0.6236, 0.5252, 0.2326, 0.5275, 0.3071, 0.2945, 0.5591, 0.5591, 1.1111, 1.1087, 1.108, 1.1075, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.1044, 1.093, 0.9576, 0.6725, 0.6504, 0.649, 0.5984, 0.5788, 0.5788, 0.5223, 0.5223, 0.4962, 0.4962, 0.4408, 0.4408, 0.4304, 0.3269, -0.2492, -0.1007]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 1, 2, 1, 3, 3, 1, 3, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 3, 1, 3, 2, 2, 2, 3, 2, 1, 2, 3, 2, 2, 1, 1, 3, 3, 2, 1, 3, 2, 2, 3, 3, 2, 3, 3, 2, 1, 2, 3, 1, 2, 2, 1, 2, 2, 3, 2, 2, 3, 1, 3, 1, 3, 3, 1, 2, 2, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, 1, 2, 3, 1, 2, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 2, 1, 3, 3], \"Freq\": [0.7766584225584098, 0.7970931413671535, 0.5241053822389735, 0.5241053822389735, 0.6133580439297603, 0.6133580439297603, 0.7123853446290241, 0.4029656701599881, 0.6044485052399822, 0.8056910644754925, 0.7766584225584098, 0.8056910357894369, 0.44257428817385447, 0.44257428817385447, 0.27702898569539397, 0.5540579713907879, 1.0617076663328284, 0.7766584225584098, 0.7766584225584098, 0.7970927857392524, 0.7970930729474347, 0.7766584225584098, 0.7744560029921204, 0.7766584225584098, 0.6328010761867112, 0.7755206212138627, 0.38776031060693134, 0.4459463911518531, 0.4459463911518531, 0.7725173755436806, 0.34851320132332775, 0.6970264026466555, 0.7970931413671535, 0.14233050456105276, 0.7116525228052638, 0.7766584225584098, 0.8056910357894369, 0.7123852063761676, 0.8887050769573506, 0.7123854658288851, 0.988613847019753, 0.4805385942392713, 0.37137779267241444, 0.37137779267241444, 0.37137779267241444, 0.7970927300756235, 0.7832301861204323, 0.7766584225584098, 0.613358011643211, 0.988614418321278, 0.8056910357894369, 0.7123852186790641, 0.5035774086233323, 0.5035774086233323, 0.7970927300756235, 0.7970927857392524, 0.8056910644754925, 0.8056910644754925, 0.7970927300756235, 0.8056910357894369, 0.8524563453936553, 0.7970927857392524, 0.6020113334270855, 0.30100566671354273, 0.9459135180770097, 0.6174726360903698, 0.6174726360903698, 0.751280285759612, 0.6174726700110157, 0.6174726700110157, 0.4168520265776051, 0.4168520265776051, 0.7970927300756235, 0.7970927857392524, 0.9306739732953799, 0.6133579022005028, 0.8056910357894369, 0.8324132155001178, 0.9475409480747405, 0.8056910357894369, 0.7766584225584098, 0.7970927857392524, 0.7970927300756235, 0.604703427950685, 0.6350090638120082, 0.5309083390395303, 0.5309083390395303, 0.7766584225584098, 0.8056910357894369, 0.6958487961831711, 1.0257587944145359, 0.9632725988864931, 0.6147355925101998, 0.7766584225584098, 0.7766584225584098, 0.5857420261694805, 0.7970931413671535, 0.5712681331108834, 0.7970927300756235, 0.7970927857392524, 1.0136116175305434, 0.9098405605163667, 0.5668965086719084, 0.41516459232604896, 0.41516459232604896, 0.8056910357894369, 0.4511417728710884, 0.4511417728710884, 0.7766584225584098, 0.8056910357894369, 0.9516169203305802, 0.39278779693488514, 0.7855755938697703, 0.6133594028821844, 0.6922498752808096, 0.9753717459420878, 0.7970927857392524, 0.7970927300756235, 0.7970927857392524, 0.6791299593752015, 0.9390239413065409, 0.9306747332761772], \"Term\": [\"absence\", \"attached\", \"author\", \"author\", \"away\", \"back\", \"better\", \"book\", \"book\", \"cast\", \"cell\", \"certain\", \"character\", \"character\", \"characters\", \"characters\", \"cheesier\", \"dealt\", \"detail\", \"development\", \"dialogue\", \"different\", \"disappointed\", \"dropping\", \"ethan\", \"even\", \"even\", \"every\", \"every\", \"feel\", \"felt\", \"felt\", \"find\", \"first\", \"first\", \"fix\", \"flaw\", \"fully\", \"get\", \"give\", \"going\", \"good\", \"got\", \"got\", \"got\", \"grasp\", \"hate\", \"highlights\", \"honest\", \"horns\", \"ignored\", \"information\", \"interesting\", \"interesting\", \"introduced\", \"island\", \"justice\", \"killers\", \"language\", \"largely\", \"lee\", \"lexie\", \"like\", \"like\", \"little\", \"love\", \"love\", \"made\", \"main\", \"main\", \"much\", \"much\", \"new\", \"oaksey\", \"obsession\", \"one\", \"overlooked\", \"pages\", \"part\", \"pevel\", \"phone\", \"possibly\", \"question\", \"read\", \"reading\", \"really\", \"really\", \"reference\", \"revelations\", \"right\", \"scene\", \"sequel\", \"short\", \"simply\", \"someone\", \"something\", \"somewhat\", \"sort\", \"speaking\", \"spent\", \"stars\", \"started\", \"still\", \"story\", \"story\", \"stronger\", \"sure\", \"sure\", \"technology\", \"thanks\", \"though\", \"time\", \"time\", \"track\", \"turns\", \"unfortunately\", \"villagers\", \"whether\", \"wife\", \"would\", \"writing\", \"xd\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el220121692600345424301132\", ldavis_el220121692600345424301132_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el220121692600345424301132\", ldavis_el220121692600345424301132_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el220121692600345424301132\", ldavis_el220121692600345424301132_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1     -0.076990 -0.061316       1        1  38.563254\n",
       "0     -0.087865  0.058677       2        1  31.658252\n",
       "2      0.164855  0.002639       3        1  29.778494, topic_info=              Term      Freq     Total Category  logprob  loglift\n",
       "316           part  5.000000  5.000000  Default  30.0000  30.0000\n",
       "373         little  5.000000  5.000000  Default  29.0000  29.0000\n",
       "271         sequel  5.000000  5.000000  Default  28.0000  28.0000\n",
       "23   unfortunately  5.000000  5.000000  Default  27.0000  27.0000\n",
       "371        writing  5.000000  5.000000  Default  26.0000  26.0000\n",
       "..             ...       ...       ...      ...      ...      ...\n",
       "315        cricket  0.433826  0.937509   Topic3  -5.9153   0.4408\n",
       "61            much  1.098649  2.398933   Topic3  -4.9861   0.4304\n",
       "58     interesting  0.819964  1.985792   Topic3  -5.2787   0.3269\n",
       "312            got  0.624965  2.692676   Topic3  -5.5502  -0.2492\n",
       "21          really  0.507181  1.883564   Topic3  -5.7591  -0.1007\n",
       "\n",
       "[146 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "317       1  0.776658   absence\n",
       "295       2  0.797093  attached\n",
       "35        1  0.524105    author\n",
       "35        2  0.524105    author\n",
       "103       1  0.613358      away\n",
       "...     ...       ...       ...\n",
       "337       2  0.797093   whether\n",
       "345       2  0.797093      wife\n",
       "65        1  0.679130     would\n",
       "371       3  0.939024   writing\n",
       "207       3  0.930675        xd\n",
       "\n",
       "[122 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_neg = pyLDAvis.gensim_models.prepare(lda_model_neg, corpus_neg, id2word_neg)\n",
    "vis_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment2_copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
